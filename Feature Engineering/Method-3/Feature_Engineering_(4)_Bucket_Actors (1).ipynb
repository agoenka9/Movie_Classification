{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering-(4)_Bucket_Actors.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4_vg14I6Jsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime,timedelta\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoDq9jhb6L6d",
        "colab_type": "code",
        "outputId": "3b859406-2b9a-4fc2-e183-35d7099fe0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhyoCsEo6Y1I",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning Movie List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCI1rhTV6N1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_excel(\"/content/Movie_dataset_3subcategories.xlsx\",sheet_name=\"All years combined\")\n",
        "#df_2019=df[df['Year']==2019][['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories']]\n",
        "df_2019=df[['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories 1','Subcategories 2','Subcategories 3']]\n",
        "df_2019=df_2019.rename(columns={\"Unnamed: 4\":\"uni_code\",\"Studio (production house)\":\"Studio\"})\n",
        "df_2019=df_2019[df_2019['uni_code']!=\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwAqWxx6esK",
        "colab_type": "code",
        "outputId": "4d445fa0-1f5d-49c6-fd37-72c4db149ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "df_2019.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018</td>\n",
              "      <td>October</td>\n",
              "      <td>5</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019</td>\n",
              "      <td>March</td>\n",
              "      <td>8</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu</td>\n",
              "      <td>Red Chillies Entertainment, Azure Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>February</td>\n",
              "      <td>9</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment, Hope P...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year   Opening  Date  ... Subcategories 1 Subcategories 2 Subcategories 3\n",
              "0  2018   October     5  ...               4               4               3\n",
              "1  2019     March     8  ...               4               4               3\n",
              "2  2019   January    11  ...               4               4               3\n",
              "3  2018     March    23  ...               4               4               3\n",
              "4  2018  February     9  ...               4               4               3\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56U6IPCU6hPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the threshold datettime\n",
        "\n",
        "df_2019['Month']=[datetime.strptime(x,\"%B\").month for x in df_2019['Opening']]\n",
        "\n",
        "df_2019['Month']=df_2019['Month'].astype(str)\n",
        "df_2019['Month']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Month']]\n",
        "df_2019['Date']=df_2019['Date'].astype(str)\n",
        "df_2019['Date']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Date']]\n",
        "\n",
        "df_2019['Datef']=df_2019['Year'].astype(str)+\"-\"+df_2019['Month'].astype(str)+\"-\"+df_2019['Date'].astype(str)\n",
        "\n",
        "df_2019['Datef']=[datetime.strptime(x,\"%Y-%m-%d\") for x in df_2019['Datef']]\n",
        "df_2019['Threshold']=[x-timedelta(days=7) for x in df_2019['Datef']]\n",
        "df_2019['lower_thres']=[x-timedelta(days=21) for x in df_2019['Datef']]\n",
        "df_2019['Week']=[x.isocalendar()[1] for x in df_2019['Datef']]\n",
        "del df_2019['Datef']\n",
        "del df_2019['Month']\n",
        "del df_2019['Year']\n",
        "\n",
        "df_2019=df_2019.fillna(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o18nyj_x689k",
        "colab_type": "code",
        "outputId": "695c660f-a839-451b-ccb6-0dff0bee9752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#parsing cast to list\n",
        "df_2019['Cast']=[x+\", \" for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.split(\",\")[0] for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.lower() for x in df_2019['Cast 1']]\n",
        "df_2019['Cast 1']=[x.strip() for x in df_2019['Cast 1']]\n",
        "\n",
        "df_2019['Cast 2']=[x.split(\",\")[1] for x in df_2019['Cast']]\n",
        "df_2019['Cast 2']=[x.lower() for x in df_2019['Cast 2']]\n",
        "df_2019['Cast 2']=[x.strip() for x in df_2019['Cast 2']]\n",
        "\n",
        "\"\"\"\n",
        "#parsing studio name to list\n",
        "df_2019['Studio']=[x.split(\",\") for x in df_2019['Studio']]\n",
        "l_new=[]\n",
        "for x in df_2019['Studio']:\n",
        "  l_new.append([p.lstrip() for p in x])\n",
        "\n",
        "df_2019['Studio']=l_new\n",
        "\n",
        "#parsing directors to list\n",
        "df_2019['Director']=[x.split(\",\") for x in df_2019['Director']]\n",
        "l_new=[]\n",
        "for x in df_2019['Director']:\n",
        "  l_new.append([p.lstrip() for p in x])\n",
        "\n",
        "df_2019['Director']=l_new\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#parsing studio name to list\\ndf_2019[\\'Studio\\']=[x.split(\",\") for x in df_2019[\\'Studio\\']]\\nl_new=[]\\nfor x in df_2019[\\'Studio\\']:\\n  l_new.append([p.lstrip() for p in x])\\n\\ndf_2019[\\'Studio\\']=l_new\\n\\n#parsing directors to list\\ndf_2019[\\'Director\\']=[x.split(\",\") for x in df_2019[\\'Director\\']]\\nl_new=[]\\nfor x in df_2019[\\'Director\\']:\\n  l_new.append([p.lstrip() for p in x])\\n\\ndf_2019[\\'Director\\']=l_new\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMCbgiHT7iIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_2019.to_csv(\"all_celeaned_f4.csv\")\n",
        "files.download(\"all_celeaned_f4.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9InEWAfP8U2H",
        "colab_type": "text"
      },
      "source": [
        "##Developing Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csx00jDS9ZPr",
        "colab_type": "code",
        "outputId": "07aa89e9-fa8e-4127-b2d2-7cf2a22c1765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#installing required packages\n",
        "pip install emot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emot in /usr/local/lib/python3.6/dist-packages (2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv5ZCKfL8auy",
        "colab_type": "code",
        "outputId": "b642243b-d71a-473d-93fd-a989d8d31391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#importing required libraries\n",
        "import nltk.data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import words\n",
        "from itertools import combinations\n",
        "import random as rd\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "#from googletrans import Translator\n",
        "from datetime import datetime\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import ast\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfKwiVx29m1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parent_list=df_2019.copy()\n",
        "L_zip =list(zip(parent_list['uni_code'],parent_list['Threshold'],parent_list['lower_thres']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZzoerNOGaIn",
        "colab_type": "code",
        "outputId": "3a1f9bba-c386-46fd-bca6-4f56b1b1de77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "source": [
        "L_zip[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2iVYI99VGaw',\n",
              "  Timestamp('2018-09-28 00:00:00'),\n",
              "  Timestamp('2018-09-14 00:00:00')),\n",
              " ('mSlgu8AQAd4',\n",
              "  Timestamp('2019-03-01 00:00:00'),\n",
              "  Timestamp('2019-02-15 00:00:00')),\n",
              " ('VVY3do673Zc',\n",
              "  Timestamp('2019-01-04 00:00:00'),\n",
              "  Timestamp('2018-12-21 00:00:00')),\n",
              " ('nLSaCFlXn-g',\n",
              "  Timestamp('2018-03-16 00:00:00'),\n",
              "  Timestamp('2018-03-02 00:00:00')),\n",
              " ('-K9ujx8vO_A',\n",
              "  Timestamp('2018-02-02 00:00:00'),\n",
              "  Timestamp('2018-01-19 00:00:00')),\n",
              " ('x_7YlGv9u1g',\n",
              "  Timestamp('2016-12-16 00:00:00'),\n",
              "  Timestamp('2016-12-02 00:00:00')),\n",
              " ('unAljCZMQYw',\n",
              "  Timestamp('2018-10-11 00:00:00'),\n",
              "  Timestamp('2018-09-27 00:00:00')),\n",
              " ('q10nfS9V090',\n",
              "  Timestamp('2019-08-08 00:00:00'),\n",
              "  Timestamp('2019-07-25 00:00:00')),\n",
              " ('vb5xCMbMfZ0',\n",
              "  Timestamp('2016-01-15 00:00:00'),\n",
              "  Timestamp('2016-01-01 00:00:00')),\n",
              " ('veJ6ejMjzgE',\n",
              "  Timestamp('2019-10-31 00:00:00'),\n",
              "  Timestamp('2019-10-17 00:00:00'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of6qbtLY_jO2",
        "colab_type": "text"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kToUEwT9vdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Declaring all necessary functions\n",
        "setofwords = set(words.words())\n",
        "\n",
        "#Convert emojis to text\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \" \" + \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "        #text = text.replace(emot,\"\")\n",
        "    return text\n",
        "\n",
        "#apply emoji convert & translate hindi to english\n",
        "def convert_lang(x,date1,date2): \n",
        "  path=\"/content/drive/My Drive/project/allmovies/\"+x+\".csv\"\n",
        "  try:\n",
        "    df=pd.read_csv(path,engine='python')\n",
        "  except:\n",
        "    df=pd.read_csv(path,engine='c')\n",
        "  del df['Unnamed: 0']\n",
        "  df=df.dropna()\n",
        "  df['pub_dates']=[datetime.strptime(x.split(\"T\")[0], \"%Y-%m-%d\") for x in df['pub_dates']]\n",
        "  comments=df[(df['pub_dates']<date1) & (df['pub_dates']>date2)]['comments']\n",
        "  comments=[convert_emojis(x) for x in comments]\n",
        "  return comments\n",
        "\n",
        "#remove punctuations, special characters and stop words from the text\n",
        "def clean_comments(comment):\n",
        "  # removing all non aplhabetic characters\n",
        "  clean_comment=re.sub(\"[^a-zA-Z]+\",\" \",comment)\n",
        "\n",
        "  #lowercase & split\n",
        "  clean_comment = clean_comment.lower()\n",
        "  clean_comment = clean_comment.split()\n",
        "\n",
        "  #remove stop words & join\n",
        "  from nltk.corpus import stopwords\n",
        "  clean_comment = [w for w in clean_comment if not w in stopwords.words(\"english\")]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  clean_comment=[lemmatizer.lemmatize(w) for w in clean_comment]\n",
        "  clean_comment=[x for x in clean_comment if x in setofwords]\n",
        "  clean_comment=\" \".join( clean_comment)\n",
        "\n",
        "  return clean_comment\n",
        "\n",
        "#get words that have highest tfidf score\n",
        "def get_topwords(cleaned_comments):\n",
        "  print(\"getting_top_words\")\n",
        "  cv=CountVectorizer(max_features=5000)\n",
        "  word_count_vector=cv.fit_transform(cleaned_comments)\n",
        "  tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "  tfidf_transformer.fit(word_count_vector)\n",
        "  df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
        "  df_idf=df_idf.sort_values(by=[\"idf_weights\"],ascending=False).iloc[0:200]\n",
        "  return list(df_idf.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYcflVh5ANV0",
        "colab_type": "text"
      },
      "source": [
        "#### Creating top 200 words for all movies combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVvXAl5M_3G2",
        "colab_type": "code",
        "outputId": "5ca8e86d-3d4e-4e09-e3bd-f9250a58f775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_vectors = pd.DataFrame(columns=['videoId','top_words'])\n",
        "L_ex=[]\n",
        "i=0\n",
        "cleaned_comm=[]\n",
        "for x,date1,date2 in L_zip:\n",
        "  print(i)\n",
        "  try:\n",
        "    proc_comments = convert_lang(x,date1,date2)\n",
        "    print(\"cleaning now\")\n",
        "    cleaned_comments=[clean_comments(comment) for comment in proc_comments]\n",
        "    cleaned_comm=cleaned_comm+[\".\".join(cleaned_comments)]\n",
        "    i=i+1\n",
        "  except:\n",
        "    print(\"excepted\")\n",
        "    print(i)\n",
        "    L_ex=L_ex+[i]\n",
        "    i=i+1\n",
        "    continue\n",
        "  \n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None, preprocessor = None, stop_words = None,max_features = 200)\n",
        "train_data_features = vectorizer.fit_transform(cleaned_comm)\n",
        "X_counts=train_data_features.toarray()\n",
        "transformer=TfidfTransformer(smooth_idf=False)\n",
        "tfidf=transformer.fit_transform(X_counts)\n",
        "X_tfidf=tfidf.toarray()\n",
        "X_tfidf2=np.where(X_tfidf>0,1,X_tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "cleaning now\n",
            "1\n",
            "cleaning now\n",
            "2\n",
            "cleaning now\n",
            "3\n",
            "cleaning now\n",
            "4\n",
            "cleaning now\n",
            "5\n",
            "cleaning now\n",
            "6\n",
            "cleaning now\n",
            "7\n",
            "cleaning now\n",
            "8\n",
            "cleaning now\n",
            "9\n",
            "cleaning now\n",
            "10\n",
            "cleaning now\n",
            "11\n",
            "cleaning now\n",
            "12\n",
            "cleaning now\n",
            "13\n",
            "cleaning now\n",
            "14\n",
            "cleaning now\n",
            "15\n",
            "cleaning now\n",
            "16\n",
            "cleaning now\n",
            "17\n",
            "cleaning now\n",
            "18\n",
            "cleaning now\n",
            "19\n",
            "cleaning now\n",
            "20\n",
            "cleaning now\n",
            "21\n",
            "cleaning now\n",
            "22\n",
            "cleaning now\n",
            "23\n",
            "cleaning now\n",
            "24\n",
            "cleaning now\n",
            "25\n",
            "cleaning now\n",
            "26\n",
            "cleaning now\n",
            "27\n",
            "cleaning now\n",
            "28\n",
            "cleaning now\n",
            "29\n",
            "cleaning now\n",
            "30\n",
            "cleaning now\n",
            "31\n",
            "cleaning now\n",
            "32\n",
            "cleaning now\n",
            "33\n",
            "cleaning now\n",
            "34\n",
            "cleaning now\n",
            "35\n",
            "cleaning now\n",
            "36\n",
            "cleaning now\n",
            "37\n",
            "cleaning now\n",
            "38\n",
            "cleaning now\n",
            "39\n",
            "cleaning now\n",
            "40\n",
            "cleaning now\n",
            "41\n",
            "cleaning now\n",
            "42\n",
            "cleaning now\n",
            "43\n",
            "cleaning now\n",
            "44\n",
            "cleaning now\n",
            "45\n",
            "cleaning now\n",
            "46\n",
            "cleaning now\n",
            "47\n",
            "cleaning now\n",
            "48\n",
            "cleaning now\n",
            "49\n",
            "cleaning now\n",
            "50\n",
            "cleaning now\n",
            "51\n",
            "cleaning now\n",
            "52\n",
            "cleaning now\n",
            "53\n",
            "cleaning now\n",
            "54\n",
            "cleaning now\n",
            "55\n",
            "cleaning now\n",
            "56\n",
            "cleaning now\n",
            "57\n",
            "cleaning now\n",
            "58\n",
            "cleaning now\n",
            "59\n",
            "cleaning now\n",
            "60\n",
            "cleaning now\n",
            "61\n",
            "cleaning now\n",
            "62\n",
            "cleaning now\n",
            "63\n",
            "cleaning now\n",
            "64\n",
            "cleaning now\n",
            "65\n",
            "cleaning now\n",
            "66\n",
            "cleaning now\n",
            "67\n",
            "cleaning now\n",
            "68\n",
            "cleaning now\n",
            "69\n",
            "cleaning now\n",
            "70\n",
            "cleaning now\n",
            "71\n",
            "cleaning now\n",
            "72\n",
            "cleaning now\n",
            "73\n",
            "cleaning now\n",
            "74\n",
            "cleaning now\n",
            "75\n",
            "cleaning now\n",
            "76\n",
            "cleaning now\n",
            "77\n",
            "cleaning now\n",
            "78\n",
            "cleaning now\n",
            "79\n",
            "cleaning now\n",
            "80\n",
            "cleaning now\n",
            "81\n",
            "cleaning now\n",
            "82\n",
            "cleaning now\n",
            "83\n",
            "cleaning now\n",
            "84\n",
            "cleaning now\n",
            "85\n",
            "cleaning now\n",
            "86\n",
            "cleaning now\n",
            "87\n",
            "cleaning now\n",
            "88\n",
            "cleaning now\n",
            "89\n",
            "cleaning now\n",
            "90\n",
            "cleaning now\n",
            "91\n",
            "cleaning now\n",
            "92\n",
            "cleaning now\n",
            "93\n",
            "cleaning now\n",
            "94\n",
            "cleaning now\n",
            "95\n",
            "cleaning now\n",
            "96\n",
            "cleaning now\n",
            "97\n",
            "cleaning now\n",
            "98\n",
            "cleaning now\n",
            "99\n",
            "cleaning now\n",
            "100\n",
            "cleaning now\n",
            "101\n",
            "cleaning now\n",
            "102\n",
            "cleaning now\n",
            "103\n",
            "cleaning now\n",
            "104\n",
            "cleaning now\n",
            "105\n",
            "cleaning now\n",
            "106\n",
            "cleaning now\n",
            "107\n",
            "cleaning now\n",
            "108\n",
            "cleaning now\n",
            "109\n",
            "cleaning now\n",
            "110\n",
            "cleaning now\n",
            "111\n",
            "cleaning now\n",
            "112\n",
            "cleaning now\n",
            "113\n",
            "excepted\n",
            "113\n",
            "114\n",
            "cleaning now\n",
            "115\n",
            "cleaning now\n",
            "116\n",
            "cleaning now\n",
            "117\n",
            "cleaning now\n",
            "118\n",
            "cleaning now\n",
            "119\n",
            "cleaning now\n",
            "120\n",
            "cleaning now\n",
            "121\n",
            "cleaning now\n",
            "122\n",
            "cleaning now\n",
            "123\n",
            "cleaning now\n",
            "124\n",
            "cleaning now\n",
            "125\n",
            "excepted\n",
            "125\n",
            "126\n",
            "cleaning now\n",
            "127\n",
            "cleaning now\n",
            "128\n",
            "cleaning now\n",
            "129\n",
            "cleaning now\n",
            "130\n",
            "cleaning now\n",
            "131\n",
            "cleaning now\n",
            "132\n",
            "cleaning now\n",
            "133\n",
            "excepted\n",
            "133\n",
            "134\n",
            "cleaning now\n",
            "135\n",
            "cleaning now\n",
            "136\n",
            "cleaning now\n",
            "137\n",
            "cleaning now\n",
            "138\n",
            "excepted\n",
            "138\n",
            "139\n",
            "cleaning now\n",
            "140\n",
            "cleaning now\n",
            "141\n",
            "cleaning now\n",
            "142\n",
            "cleaning now\n",
            "143\n",
            "cleaning now\n",
            "144\n",
            "cleaning now\n",
            "145\n",
            "cleaning now\n",
            "146\n",
            "cleaning now\n",
            "147\n",
            "excepted\n",
            "147\n",
            "148\n",
            "cleaning now\n",
            "149\n",
            "excepted\n",
            "149\n",
            "150\n",
            "cleaning now\n",
            "151\n",
            "cleaning now\n",
            "152\n",
            "excepted\n",
            "152\n",
            "153\n",
            "cleaning now\n",
            "154\n",
            "cleaning now\n",
            "155\n",
            "cleaning now\n",
            "156\n",
            "cleaning now\n",
            "157\n",
            "cleaning now\n",
            "158\n",
            "cleaning now\n",
            "159\n",
            "cleaning now\n",
            "160\n",
            "cleaning now\n",
            "161\n",
            "cleaning now\n",
            "162\n",
            "cleaning now\n",
            "163\n",
            "cleaning now\n",
            "164\n",
            "cleaning now\n",
            "165\n",
            "cleaning now\n",
            "166\n",
            "cleaning now\n",
            "167\n",
            "excepted\n",
            "167\n",
            "168\n",
            "cleaning now\n",
            "169\n",
            "excepted\n",
            "169\n",
            "170\n",
            "cleaning now\n",
            "171\n",
            "cleaning now\n",
            "172\n",
            "cleaning now\n",
            "173\n",
            "cleaning now\n",
            "174\n",
            "cleaning now\n",
            "175\n",
            "cleaning now\n",
            "176\n",
            "cleaning now\n",
            "177\n",
            "cleaning now\n",
            "178\n",
            "cleaning now\n",
            "179\n",
            "cleaning now\n",
            "180\n",
            "cleaning now\n",
            "181\n",
            "cleaning now\n",
            "182\n",
            "cleaning now\n",
            "183\n",
            "cleaning now\n",
            "184\n",
            "cleaning now\n",
            "185\n",
            "cleaning now\n",
            "186\n",
            "cleaning now\n",
            "187\n",
            "cleaning now\n",
            "188\n",
            "cleaning now\n",
            "189\n",
            "cleaning now\n",
            "190\n",
            "cleaning now\n",
            "191\n",
            "excepted\n",
            "191\n",
            "192\n",
            "cleaning now\n",
            "193\n",
            "cleaning now\n",
            "194\n",
            "cleaning now\n",
            "195\n",
            "excepted\n",
            "195\n",
            "196\n",
            "cleaning now\n",
            "197\n",
            "cleaning now\n",
            "198\n",
            "cleaning now\n",
            "199\n",
            "cleaning now\n",
            "200\n",
            "cleaning now\n",
            "201\n",
            "cleaning now\n",
            "202\n",
            "cleaning now\n",
            "203\n",
            "cleaning now\n",
            "204\n",
            "cleaning now\n",
            "205\n",
            "cleaning now\n",
            "206\n",
            "cleaning now\n",
            "207\n",
            "cleaning now\n",
            "208\n",
            "excepted\n",
            "208\n",
            "209\n",
            "cleaning now\n",
            "210\n",
            "cleaning now\n",
            "211\n",
            "cleaning now\n",
            "212\n",
            "cleaning now\n",
            "213\n",
            "cleaning now\n",
            "214\n",
            "cleaning now\n",
            "215\n",
            "cleaning now\n",
            "216\n",
            "cleaning now\n",
            "217\n",
            "cleaning now\n",
            "218\n",
            "cleaning now\n",
            "219\n",
            "cleaning now\n",
            "220\n",
            "cleaning now\n",
            "221\n",
            "cleaning now\n",
            "222\n",
            "cleaning now\n",
            "223\n",
            "cleaning now\n",
            "224\n",
            "cleaning now\n",
            "225\n",
            "cleaning now\n",
            "226\n",
            "cleaning now\n",
            "227\n",
            "cleaning now\n",
            "228\n",
            "cleaning now\n",
            "229\n",
            "cleaning now\n",
            "230\n",
            "cleaning now\n",
            "231\n",
            "cleaning now\n",
            "232\n",
            "cleaning now\n",
            "233\n",
            "cleaning now\n",
            "234\n",
            "cleaning now\n",
            "235\n",
            "cleaning now\n",
            "236\n",
            "cleaning now\n",
            "237\n",
            "cleaning now\n",
            "238\n",
            "cleaning now\n",
            "239\n",
            "cleaning now\n",
            "240\n",
            "cleaning now\n",
            "241\n",
            "cleaning now\n",
            "242\n",
            "cleaning now\n",
            "243\n",
            "cleaning now\n",
            "244\n",
            "cleaning now\n",
            "245\n",
            "cleaning now\n",
            "246\n",
            "cleaning now\n",
            "247\n",
            "cleaning now\n",
            "248\n",
            "cleaning now\n",
            "249\n",
            "cleaning now\n",
            "250\n",
            "cleaning now\n",
            "251\n",
            "cleaning now\n",
            "252\n",
            "cleaning now\n",
            "253\n",
            "cleaning now\n",
            "254\n",
            "cleaning now\n",
            "255\n",
            "cleaning now\n",
            "256\n",
            "cleaning now\n",
            "257\n",
            "cleaning now\n",
            "258\n",
            "cleaning now\n",
            "259\n",
            "cleaning now\n",
            "260\n",
            "cleaning now\n",
            "261\n",
            "cleaning now\n",
            "262\n",
            "cleaning now\n",
            "263\n",
            "cleaning now\n",
            "264\n",
            "cleaning now\n",
            "265\n",
            "cleaning now\n",
            "266\n",
            "cleaning now\n",
            "267\n",
            "cleaning now\n",
            "268\n",
            "cleaning now\n",
            "269\n",
            "cleaning now\n",
            "270\n",
            "cleaning now\n",
            "271\n",
            "cleaning now\n",
            "272\n",
            "cleaning now\n",
            "273\n",
            "cleaning now\n",
            "274\n",
            "cleaning now\n",
            "275\n",
            "cleaning now\n",
            "276\n",
            "cleaning now\n",
            "277\n",
            "cleaning now\n",
            "278\n",
            "cleaning now\n",
            "279\n",
            "cleaning now\n",
            "280\n",
            "cleaning now\n",
            "281\n",
            "cleaning now\n",
            "282\n",
            "cleaning now\n",
            "283\n",
            "cleaning now\n",
            "284\n",
            "cleaning now\n",
            "285\n",
            "cleaning now\n",
            "286\n",
            "cleaning now\n",
            "287\n",
            "cleaning now\n",
            "288\n",
            "cleaning now\n",
            "289\n",
            "cleaning now\n",
            "290\n",
            "cleaning now\n",
            "291\n",
            "cleaning now\n",
            "292\n",
            "cleaning now\n",
            "293\n",
            "cleaning now\n",
            "294\n",
            "cleaning now\n",
            "295\n",
            "cleaning now\n",
            "296\n",
            "cleaning now\n",
            "297\n",
            "cleaning now\n",
            "298\n",
            "cleaning now\n",
            "299\n",
            "cleaning now\n",
            "300\n",
            "cleaning now\n",
            "301\n",
            "cleaning now\n",
            "302\n",
            "cleaning now\n",
            "303\n",
            "cleaning now\n",
            "304\n",
            "cleaning now\n",
            "305\n",
            "cleaning now\n",
            "306\n",
            "cleaning now\n",
            "307\n",
            "cleaning now\n",
            "308\n",
            "cleaning now\n",
            "309\n",
            "cleaning now\n",
            "310\n",
            "cleaning now\n",
            "311\n",
            "cleaning now\n",
            "312\n",
            "cleaning now\n",
            "313\n",
            "cleaning now\n",
            "314\n",
            "cleaning now\n",
            "315\n",
            "cleaning now\n",
            "316\n",
            "cleaning now\n",
            "317\n",
            "cleaning now\n",
            "318\n",
            "cleaning now\n",
            "319\n",
            "cleaning now\n",
            "320\n",
            "cleaning now\n",
            "321\n",
            "cleaning now\n",
            "322\n",
            "cleaning now\n",
            "323\n",
            "cleaning now\n",
            "324\n",
            "cleaning now\n",
            "325\n",
            "cleaning now\n",
            "326\n",
            "cleaning now\n",
            "327\n",
            "cleaning now\n",
            "328\n",
            "cleaning now\n",
            "329\n",
            "cleaning now\n",
            "330\n",
            "cleaning now\n",
            "331\n",
            "cleaning now\n",
            "332\n",
            "cleaning now\n",
            "333\n",
            "cleaning now\n",
            "334\n",
            "cleaning now\n",
            "335\n",
            "cleaning now\n",
            "336\n",
            "cleaning now\n",
            "337\n",
            "cleaning now\n",
            "338\n",
            "cleaning now\n",
            "339\n",
            "cleaning now\n",
            "340\n",
            "cleaning now\n",
            "341\n",
            "cleaning now\n",
            "342\n",
            "cleaning now\n",
            "343\n",
            "cleaning now\n",
            "344\n",
            "cleaning now\n",
            "345\n",
            "cleaning now\n",
            "346\n",
            "cleaning now\n",
            "347\n",
            "cleaning now\n",
            "348\n",
            "cleaning now\n",
            "349\n",
            "cleaning now\n",
            "350\n",
            "cleaning now\n",
            "351\n",
            "cleaning now\n",
            "352\n",
            "cleaning now\n",
            "353\n",
            "cleaning now\n",
            "354\n",
            "cleaning now\n",
            "355\n",
            "cleaning now\n",
            "356\n",
            "cleaning now\n",
            "357\n",
            "cleaning now\n",
            "358\n",
            "cleaning now\n",
            "359\n",
            "cleaning now\n",
            "360\n",
            "cleaning now\n",
            "361\n",
            "cleaning now\n",
            "362\n",
            "cleaning now\n",
            "363\n",
            "excepted\n",
            "363\n",
            "364\n",
            "excepted\n",
            "364\n",
            "365\n",
            "cleaning now\n",
            "366\n",
            "cleaning now\n",
            "367\n",
            "cleaning now\n",
            "368\n",
            "cleaning now\n",
            "369\n",
            "cleaning now\n",
            "370\n",
            "cleaning now\n",
            "371\n",
            "cleaning now\n",
            "372\n",
            "cleaning now\n",
            "373\n",
            "cleaning now\n",
            "374\n",
            "cleaning now\n",
            "375\n",
            "cleaning now\n",
            "376\n",
            "cleaning now\n",
            "377\n",
            "cleaning now\n",
            "378\n",
            "cleaning now\n",
            "379\n",
            "excepted\n",
            "379\n",
            "380\n",
            "cleaning now\n",
            "381\n",
            "cleaning now\n",
            "382\n",
            "cleaning now\n",
            "383\n",
            "cleaning now\n",
            "384\n",
            "cleaning now\n",
            "385\n",
            "cleaning now\n",
            "386\n",
            "cleaning now\n",
            "387\n",
            "excepted\n",
            "387\n",
            "388\n",
            "cleaning now\n",
            "389\n",
            "cleaning now\n",
            "390\n",
            "excepted\n",
            "390\n",
            "391\n",
            "cleaning now\n",
            "392\n",
            "cleaning now\n",
            "393\n",
            "cleaning now\n",
            "394\n",
            "cleaning now\n",
            "395\n",
            "cleaning now\n",
            "396\n",
            "excepted\n",
            "396\n",
            "397\n",
            "cleaning now\n",
            "398\n",
            "excepted\n",
            "398\n",
            "399\n",
            "cleaning now\n",
            "400\n",
            "cleaning now\n",
            "401\n",
            "cleaning now\n",
            "402\n",
            "cleaning now\n",
            "403\n",
            "cleaning now\n",
            "404\n",
            "cleaning now\n",
            "405\n",
            "cleaning now\n",
            "406\n",
            "cleaning now\n",
            "407\n",
            "excepted\n",
            "407\n",
            "408\n",
            "cleaning now\n",
            "409\n",
            "excepted\n",
            "409\n",
            "410\n",
            "cleaning now\n",
            "411\n",
            "cleaning now\n",
            "412\n",
            "cleaning now\n",
            "413\n",
            "cleaning now\n",
            "414\n",
            "excepted\n",
            "414\n",
            "415\n",
            "cleaning now\n",
            "416\n",
            "cleaning now\n",
            "417\n",
            "excepted\n",
            "417\n",
            "418\n",
            "cleaning now\n",
            "419\n",
            "cleaning now\n",
            "420\n",
            "cleaning now\n",
            "421\n",
            "cleaning now\n",
            "422\n",
            "cleaning now\n",
            "423\n",
            "excepted\n",
            "423\n",
            "424\n",
            "cleaning now\n",
            "425\n",
            "cleaning now\n",
            "426\n",
            "excepted\n",
            "426\n",
            "427\n",
            "cleaning now\n",
            "428\n",
            "cleaning now\n",
            "429\n",
            "cleaning now\n",
            "430\n",
            "cleaning now\n",
            "431\n",
            "cleaning now\n",
            "432\n",
            "cleaning now\n",
            "433\n",
            "excepted\n",
            "433\n",
            "434\n",
            "cleaning now\n",
            "435\n",
            "cleaning now\n",
            "436\n",
            "excepted\n",
            "436\n",
            "437\n",
            "excepted\n",
            "437\n",
            "438\n",
            "excepted\n",
            "438\n",
            "439\n",
            "cleaning now\n",
            "440\n",
            "cleaning now\n",
            "441\n",
            "cleaning now\n",
            "442\n",
            "cleaning now\n",
            "443\n",
            "cleaning now\n",
            "444\n",
            "cleaning now\n",
            "445\n",
            "excepted\n",
            "445\n",
            "446\n",
            "cleaning now\n",
            "447\n",
            "cleaning now\n",
            "448\n",
            "cleaning now\n",
            "449\n",
            "cleaning now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTnqhkF0JRNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None, preprocessor = None, stop_words = None,max_features = 700)\n",
        "train_data_features = vectorizer.fit_transform(cleaned_comm)\n",
        "X_counts=train_data_features.toarray()\n",
        "transformer=TfidfTransformer(smooth_idf=False)\n",
        "tfidf=transformer.fit_transform(X_counts)\n",
        "X_tfidf=tfidf.toarray()\n",
        "X_tfidf2=np.where(X_tfidf>0,1,X_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgTYOFixLebe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dump=pd.DataFrame()\n",
        "dump['dumpf4']=cleaned_comm\n",
        "dump.to_csv(\"dump_f4_final.csv\")\n",
        "#files.download(\"dump_f4.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_883vazaNhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"dump_f4_final.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slaf9hWCfwIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open('f1.txt','w')\n",
        "for ele in L_ex:\n",
        "    f.write(str(ele)+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw3UNVn9f2h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"f1.txt\n",
        "\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrPyiOyfhryZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "dump=pd.read_csv(\"/content/dump_f4 (1).csv\")\n",
        "dump=dump.fillna(\"\")\n",
        "cleaned_comm=list(dump['dumpf4'])\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDavqSnKAUwY",
        "colab_type": "code",
        "outputId": "08ac91a9-b7e9-4d39-d08e-3abf88246239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "for_tensor=df_2019.copy()\n",
        "for_tensor=for_tensor.drop(index=L_ex)\n",
        "#the following are needed when read from csv directly\n",
        "#for_tensor['Cast']=[ast.literal_eval(x) for x in for_tensor['Cast']]\n",
        "#for_tensor['Studio']=[ast.literal_eval(x) for x in for_tensor['Studio']]\n",
        "#for_tensor['Director']=[ast.literal_eval(x) for x in for_tensor['Director']]\n",
        "for_tensor.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>lower_thres</th>\n",
              "      <th>Week</th>\n",
              "      <th>Cast 1</th>\n",
              "      <th>Cast 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>October</td>\n",
              "      <td>05</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte,</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>2018-09-14</td>\n",
              "      <td>40</td>\n",
              "      <td>ayushman khurana</td>\n",
              "      <td>tabu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>March</td>\n",
              "      <td>08</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu,</td>\n",
              "      <td>Red Chillies Entertainment, Azure Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>2019-02-15</td>\n",
              "      <td>10</td>\n",
              "      <td>amitabh bachchan</td>\n",
              "      <td>taapsee pannu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>2018-12-21</td>\n",
              "      <td>2</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>mohit raina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji,</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16</td>\n",
              "      <td>2018-03-02</td>\n",
              "      <td>12</td>\n",
              "      <td>rani mukerji</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>February</td>\n",
              "      <td>09</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte,</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment, Hope P...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>2018-01-19</td>\n",
              "      <td>6</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>sonam kapoor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Opening Date  ...            Cast 1         Cast 2\n",
              "0   October   05  ...  ayushman khurana           tabu\n",
              "1     March   08  ...  amitabh bachchan  taapsee pannu\n",
              "2   January   11  ...     vicky kaushal    mohit raina\n",
              "3     March   23  ...      rani mukerji               \n",
              "4  February   09  ...      akshay kumar   sonam kapoor\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_vQ4v3Aqpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cast=pd.read_csv(\"/content/actor-cat.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GWsGSYsMUVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys=list(df_cast['Cast'])\n",
        "keys=keys+[\"\"]\n",
        "vals=list(df_cast['category'])\n",
        "vals=vals+[0]\n",
        "dict_cov=dict(zip(keys,vals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vCq3PGXm9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_t2=for_tensor.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hctTObZAXpLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_t2['Cast 1s']=[dict_cov[x] for x in for_t2['Cast 1']]\n",
        "for_t2['Cast 2s']=[dict_cov[x] for x in for_t2['Cast 2']]\n",
        "for_t2.head()\n",
        "for_t3=for_t2[['Title', 'uni_code']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsCCgSfQYMgu",
        "colab_type": "code",
        "outputId": "e44a618c-c9ed-4f8b-e47b-cc837df8660d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "d={1: [0]*len(for_t2),2: [0]*len(for_t2),3: [0]*len(for_t2),4: [0]*len(for_t2),0: [0]*len(for_t2)}\n",
        "\n",
        "#print(d)\n",
        "for i in range(0,len(for_t2)):\n",
        "  #print(i)\n",
        "  k1=for_t2['Cast 1s'].iloc[i]\n",
        "  k2=for_t2['Cast 2s'].iloc[i]\n",
        "  #print(d[k1][i])\n",
        "  d[k1][i]=d[k1][i]+1\n",
        "  d[k2][i]=d[k2][i]+1\n",
        "  #d[for_t2['Cast 2s'].iloc[i]][i]=d[for_t2['Cast 2s'].iloc[i]][i]+1\n",
        "  #print(d)\n",
        "\n",
        "\n",
        "for_t3['a_cat1']=d[1]\n",
        "for_t3['a_cat2']=d[2]\n",
        "for_t3['a_cat3']=d[3]\n",
        "for_t3['a_cat4']=d[4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EBZdfPCY-GU",
        "colab_type": "code",
        "outputId": "523eb94d-f844-4a22-b727-4e6db4b5b2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "for_t2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>lower_thres</th>\n",
              "      <th>Week</th>\n",
              "      <th>Cast 1</th>\n",
              "      <th>Cast 2</th>\n",
              "      <th>Cast 1s</th>\n",
              "      <th>Cast 2s</th>\n",
              "      <th>a_cat1</th>\n",
              "      <th>a_cat2</th>\n",
              "      <th>a_cat3</th>\n",
              "      <th>a_cat4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>October</td>\n",
              "      <td>05</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte,</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>2018-09-14</td>\n",
              "      <td>40</td>\n",
              "      <td>ayushman khurana</td>\n",
              "      <td>tabu</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>March</td>\n",
              "      <td>08</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu,</td>\n",
              "      <td>Red Chillies Entertainment, Azure Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>2019-02-15</td>\n",
              "      <td>10</td>\n",
              "      <td>amitabh bachchan</td>\n",
              "      <td>taapsee pannu</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>2018-12-21</td>\n",
              "      <td>2</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>mohit raina</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji,</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16</td>\n",
              "      <td>2018-03-02</td>\n",
              "      <td>12</td>\n",
              "      <td>rani mukerji</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>February</td>\n",
              "      <td>09</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte,</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment, Hope P...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>2018-01-19</td>\n",
              "      <td>6</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>sonam kapoor</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Opening Date                     Title  ... a_cat2 a_cat3 a_cat4\n",
              "0   October   05                 Andhadhun  ...      1      0      1\n",
              "1     March   08                     Badla  ...      1      0      0\n",
              "2   January   11  Uri: The Surgical Strike  ...      0      1      1\n",
              "3     March   23                    Hichki  ...      1      0      0\n",
              "4  February   09                   Pad Man  ...      0      0      0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY-HF9m2Ymzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_cal={}\n",
        "for i in range(1,53):\n",
        "  d_cal.update({i:[0]*len(for_t2)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixBHJ1RUdjVc",
        "colab_type": "code",
        "outputId": "8450705b-5acc-4f72-f786-e02a26ab56eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for i in range(0,len(for_t2)):\n",
        "  #print(i)\n",
        "  k=for_t2['Week'].iloc[i]\n",
        "  #print(d[k1][i])\n",
        "  d_cal[k][i]=d_cal[k][i]+1\n",
        "  #d[k2][i]=d[k2][i]+1\n",
        "  #d[for_t2['Cast 2s'].iloc[i]][i]=d[for_t2['Cast 2s'].iloc[i]][i]+1\n",
        "  #print(d)\n",
        "\n",
        "for i in range(1,53):\n",
        "  name=\"Week \"+str(i)\n",
        "  for_t3[name]=d_cal[i]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNDBFJE-dNea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_t3.head()\n",
        "del for_t3['Title']\n",
        "del for_t3['uni_code']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-cyPsbtOBLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_tensor['Cast 1']=[dict_cov[x] for x in for_tensor['Cast 1']]\n",
        "for_tensor['Cast 2']=[dict_cov[x] for x in for_tensor['Cast 2']]\n",
        "for_tensor['Cast score']=for_tensor['Cast 1']+for_tensor['Cast 2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDJcpdrwOR9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_mat=for_tensor[['Cast score','Week']]\n",
        "for_mat=for_mat.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhlfVDie3e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_mat2=for_t3.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHcoNJCKNVOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_t=np.concatenate((X_tfidf2,for_mat2),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br01Dx2SriUX",
        "colab_type": "code",
        "outputId": "726beb24-a39e-46b7-8f30-cccc82ec0128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_tfidf2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420, 700)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi582A9zPfHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_result1 = np.array(for_tensor['Subcategories 1'])\n",
        "cat_result2 = np.array(for_tensor['Subcategories 2'])\n",
        "cat_result3 = np.array(for_tensor['Subcategories 3'])\n",
        "#org_mat = mat.toarray()\n",
        "org_mat_df = pd.DataFrame(data=s_t, columns=['c'+str(i) for i in range(s_t.shape[1])])\n",
        "org_mat_df.to_csv(\"/content/drive/My Drive/project/Results/F4_model_data_700_catex.csv\")\n",
        "cat_result_df1 = pd.DataFrame({'t': cat_result1})\n",
        "cat_result_df1.to_csv(\"/content/drive/My Drive/project/Results/F4_model_data_target_1.csv\")\n",
        "cat_result_df2 = pd.DataFrame({'t': cat_result2})\n",
        "cat_result_df2.to_csv(\"/content/drive/My Drive/project/Results/F4_model_data_target_2.csv\")\n",
        "cat_result_df3 = pd.DataFrame({'t': cat_result3})\n",
        "cat_result_df3.to_csv(\"/content/drive/My Drive/project/Results/F4_model_data_target_3.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}