{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZHn2ijfKtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime,timedelta\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G3rW0FvfX3H",
        "colab_type": "code",
        "outputId": "e1383748-6897-4661-dab4-8a9ed50cfbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjR-6ASbfcHH",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Movie List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62YrnI0nffOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_excel(\"/content/Movie_dataset_3subcategories.xlsx\",sheet_name=\"All years combined\")\n",
        "#df_2019=df[df['Year']==2019][['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories']]\n",
        "df_2019=df[['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories 1','Subcategories 2','Subcategories 3']]\n",
        "df_2019=df_2019.rename(columns={\"Unnamed: 4\":\"uni_code\",\"Studio (production house)\":\"Studio\"})\n",
        "df_2019=df_2019[df_2019['uni_code']!=\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih6OXJswfoK5",
        "colab_type": "code",
        "outputId": "1ed60843-9ca4-48ef-e587-a58982f7fde4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "df_2019.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018</td>\n",
              "      <td>October</td>\n",
              "      <td>5</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019</td>\n",
              "      <td>March</td>\n",
              "      <td>8</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu</td>\n",
              "      <td>Red Chillies Entertainment, Azure Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>February</td>\n",
              "      <td>9</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment, Hope P...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year   Opening  Date  ... Subcategories 1 Subcategories 2 Subcategories 3\n",
              "0  2018   October     5  ...               4               4               3\n",
              "1  2019     March     8  ...               4               4               3\n",
              "2  2019   January    11  ...               4               4               3\n",
              "3  2018     March    23  ...               4               4               3\n",
              "4  2018  February     9  ...               4               4               3\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1DeX_5YfqpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the threshold datettime\n",
        "\n",
        "df_2019['Month']=[datetime.strptime(x,\"%B\").month for x in df_2019['Opening']]\n",
        "\n",
        "df_2019['Month']=df_2019['Month'].astype(str)\n",
        "df_2019['Month']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Month']]\n",
        "df_2019['Date']=df_2019['Date'].astype(str)\n",
        "df_2019['Date']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Date']]\n",
        "\n",
        "df_2019['Datef']=df_2019['Year'].astype(str)+\"-\"+df_2019['Month'].astype(str)+\"-\"+df_2019['Date'].astype(str)\n",
        "\n",
        "df_2019['Datef']=[datetime.strptime(x,\"%Y-%m-%d\") for x in df_2019['Datef']]\n",
        "df_2019['Threshold']=[x-timedelta(days=7) for x in df_2019['Datef']]\n",
        "df_2019['lower_thres']=[x-timedelta(days=21) for x in df_2019['Datef']]\n",
        "df_2019['Week']=[x.isocalendar()[1] for x in df_2019['Datef']]\n",
        "del df_2019['Datef']\n",
        "del df_2019['Month']\n",
        "del df_2019['Year']\n",
        "\n",
        "df_2019=df_2019.fillna(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxXdaXq2fu6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parsing cast,director,studio to list\n",
        "df_2019['Cast']=[x+\", \" for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.split(\",\")[0] for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.lower() for x in df_2019['Cast 1']]\n",
        "df_2019['Cast 1']=[x.strip() for x in df_2019['Cast 1']]\n",
        "\n",
        "df_2019['Cast 2']=[x.split(\",\")[1] for x in df_2019['Cast']]\n",
        "df_2019['Cast 2']=[x.lower() for x in df_2019['Cast 2']]\n",
        "df_2019['Cast 2']=[x.strip() for x in df_2019['Cast 2']]\n",
        "\n",
        "df_2019['Director']=[x.split(\",\")[0] for x in df_2019['Director']]\n",
        "df_2019['Studio']=[x.split(\",\")[0] for x in df_2019['Studio']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNjqKbJBgTQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_2019.to_csv(\"all_celeaned_model-1.csv\")\n",
        "files.download(\"all_celeaned_model-1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Ck1MbKgXB4",
        "colab_type": "text"
      },
      "source": [
        "## Developing Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYLJBQOPga2I",
        "colab_type": "code",
        "outputId": "3d59616d-f782-457a-f4b3-1f5d2d74b747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#installing required packages\n",
        "pip install emot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emot in /usr/local/lib/python3.6/dist-packages (2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpzvqx5gcaE",
        "colab_type": "code",
        "outputId": "c6d299fa-7a65-4ccf-a87f-2febab3ab710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#importing required libraries\n",
        "import nltk.data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import words\n",
        "from itertools import combinations\n",
        "import random as rd\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "#from googletrans import Translator\n",
        "from datetime import datetime\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import ast\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nTAOkNvgegN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parent_list=df_2019.copy()\n",
        "L_zip =list(zip(parent_list['uni_code'],parent_list['Threshold'],parent_list['lower_thres']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytn_-YqUgi7-",
        "colab_type": "text"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERR1EqqFggcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Declaring all necessary functions\n",
        "setofwords = set(words.words())\n",
        "\n",
        "#Convert emojis to text\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \" \" + \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "        #text = text.replace(emot,\"\")\n",
        "    return text\n",
        "\n",
        "#apply emoji convert & translate hindi to english\n",
        "def convert_lang(x,date1,date2): \n",
        "  path=\"/content/drive/My Drive/project/allmovies/\"+x+\".csv\"\n",
        "  try:\n",
        "    df=pd.read_csv(path,engine='python')\n",
        "  except:\n",
        "    df=pd.read_csv(path,engine='c')\n",
        "  del df['Unnamed: 0']\n",
        "  df=df.dropna()\n",
        "  df['pub_dates']=[datetime.strptime(x.split(\"T\")[0], \"%Y-%m-%d\") for x in df['pub_dates']]\n",
        "  comments=df[(df['pub_dates']<date1) & (df['pub_dates']>date2)]['comments']\n",
        "  comments=[convert_emojis(x) for x in comments]\n",
        "  return comments\n",
        "\n",
        "#remove punctuations, special characters and stop words from the text\n",
        "def clean_comments(comment):\n",
        "  # removing all non aplhabetic characters\n",
        "  clean_comment=re.sub(\"[^a-zA-Z]+\",\" \",comment)\n",
        "\n",
        "  #lowercase & split\n",
        "  clean_comment = clean_comment.lower()\n",
        "  clean_comment = clean_comment.split()\n",
        "\n",
        "  #remove stop words & join\n",
        "  from nltk.corpus import stopwords\n",
        "  clean_comment = [w for w in clean_comment if not w in stopwords.words(\"english\")]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  clean_comment=[lemmatizer.lemmatize(w) for w in clean_comment]\n",
        "  clean_comment=[x for x in clean_comment if x in setofwords]\n",
        "  clean_comment=\" \".join( clean_comment)\n",
        "\n",
        "  return clean_comment\n",
        "\n",
        "#get words that have highest tfidf score\n",
        "def get_topwords(cleaned_comments):\n",
        "  print(\"getting_top_words\")\n",
        "  cv=CountVectorizer(max_features=5000)\n",
        "  word_count_vector=cv.fit_transform(cleaned_comments)\n",
        "  tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "  tfidf_transformer.fit(word_count_vector)\n",
        "  df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
        "  df_idf=df_idf.sort_values(by=[\"idf_weights\"],ascending=False).iloc[0:200]\n",
        "  return list(df_idf.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKOrVovIgoxp",
        "colab_type": "code",
        "outputId": "46edec1c-f81a-4067-b4e2-59652a317b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_vectors = pd.DataFrame(columns=['videoId','top_words'])\n",
        "i=0\n",
        "for x,date1,date2 in L_zip:\n",
        "  print(i)\n",
        "  try:\n",
        "    proc_comments = convert_lang(x,date1,date2)\n",
        "    print(\"cleaning now\")\n",
        "    cleaned_comments=[clean_comments(comment) for comment in proc_comments]\n",
        "    final_words=get_topwords(cleaned_comments)\n",
        "    final_vectors = final_vectors.append({'videoId': x , 'top_words':final_words}, ignore_index=True)\n",
        "    #cleaned_comm=cleaned_comm+[\".\".join(cleaned_comments)]\n",
        "  except:\n",
        "    print(\"excepted\")\n",
        "    #print(i)\n",
        "    continue\n",
        "  i=i+1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "cleaning now\n",
            "getting_top_words\n",
            "1\n",
            "cleaning now\n",
            "getting_top_words\n",
            "2\n",
            "cleaning now\n",
            "getting_top_words\n",
            "3\n",
            "cleaning now\n",
            "getting_top_words\n",
            "4\n",
            "cleaning now\n",
            "getting_top_words\n",
            "5\n",
            "cleaning now\n",
            "getting_top_words\n",
            "6\n",
            "cleaning now\n",
            "getting_top_words\n",
            "7\n",
            "cleaning now\n",
            "getting_top_words\n",
            "8\n",
            "cleaning now\n",
            "getting_top_words\n",
            "9\n",
            "cleaning now\n",
            "getting_top_words\n",
            "10\n",
            "cleaning now\n",
            "getting_top_words\n",
            "11\n",
            "cleaning now\n",
            "getting_top_words\n",
            "12\n",
            "cleaning now\n",
            "getting_top_words\n",
            "13\n",
            "cleaning now\n",
            "getting_top_words\n",
            "14\n",
            "cleaning now\n",
            "getting_top_words\n",
            "15\n",
            "cleaning now\n",
            "getting_top_words\n",
            "16\n",
            "cleaning now\n",
            "getting_top_words\n",
            "17\n",
            "cleaning now\n",
            "getting_top_words\n",
            "18\n",
            "cleaning now\n",
            "getting_top_words\n",
            "19\n",
            "cleaning now\n",
            "getting_top_words\n",
            "20\n",
            "cleaning now\n",
            "getting_top_words\n",
            "21\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "21\n",
            "cleaning now\n",
            "getting_top_words\n",
            "22\n",
            "cleaning now\n",
            "getting_top_words\n",
            "23\n",
            "cleaning now\n",
            "getting_top_words\n",
            "24\n",
            "cleaning now\n",
            "getting_top_words\n",
            "25\n",
            "cleaning now\n",
            "getting_top_words\n",
            "26\n",
            "cleaning now\n",
            "getting_top_words\n",
            "27\n",
            "cleaning now\n",
            "getting_top_words\n",
            "28\n",
            "cleaning now\n",
            "getting_top_words\n",
            "29\n",
            "cleaning now\n",
            "getting_top_words\n",
            "30\n",
            "cleaning now\n",
            "getting_top_words\n",
            "31\n",
            "cleaning now\n",
            "getting_top_words\n",
            "32\n",
            "cleaning now\n",
            "getting_top_words\n",
            "33\n",
            "cleaning now\n",
            "getting_top_words\n",
            "34\n",
            "cleaning now\n",
            "getting_top_words\n",
            "35\n",
            "cleaning now\n",
            "getting_top_words\n",
            "36\n",
            "cleaning now\n",
            "getting_top_words\n",
            "37\n",
            "cleaning now\n",
            "getting_top_words\n",
            "38\n",
            "cleaning now\n",
            "getting_top_words\n",
            "39\n",
            "cleaning now\n",
            "getting_top_words\n",
            "40\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "40\n",
            "cleaning now\n",
            "getting_top_words\n",
            "41\n",
            "cleaning now\n",
            "getting_top_words\n",
            "42\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "42\n",
            "cleaning now\n",
            "getting_top_words\n",
            "43\n",
            "cleaning now\n",
            "getting_top_words\n",
            "44\n",
            "cleaning now\n",
            "getting_top_words\n",
            "45\n",
            "cleaning now\n",
            "getting_top_words\n",
            "46\n",
            "cleaning now\n",
            "getting_top_words\n",
            "47\n",
            "cleaning now\n",
            "getting_top_words\n",
            "48\n",
            "cleaning now\n",
            "getting_top_words\n",
            "49\n",
            "cleaning now\n",
            "getting_top_words\n",
            "50\n",
            "cleaning now\n",
            "getting_top_words\n",
            "51\n",
            "cleaning now\n",
            "getting_top_words\n",
            "52\n",
            "cleaning now\n",
            "getting_top_words\n",
            "53\n",
            "cleaning now\n",
            "getting_top_words\n",
            "54\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "54\n",
            "cleaning now\n",
            "getting_top_words\n",
            "55\n",
            "cleaning now\n",
            "getting_top_words\n",
            "56\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "56\n",
            "cleaning now\n",
            "getting_top_words\n",
            "57\n",
            "cleaning now\n",
            "getting_top_words\n",
            "58\n",
            "cleaning now\n",
            "getting_top_words\n",
            "59\n",
            "cleaning now\n",
            "getting_top_words\n",
            "60\n",
            "cleaning now\n",
            "getting_top_words\n",
            "61\n",
            "cleaning now\n",
            "getting_top_words\n",
            "62\n",
            "cleaning now\n",
            "getting_top_words\n",
            "63\n",
            "cleaning now\n",
            "getting_top_words\n",
            "64\n",
            "cleaning now\n",
            "getting_top_words\n",
            "65\n",
            "cleaning now\n",
            "getting_top_words\n",
            "66\n",
            "cleaning now\n",
            "getting_top_words\n",
            "67\n",
            "cleaning now\n",
            "getting_top_words\n",
            "68\n",
            "cleaning now\n",
            "getting_top_words\n",
            "69\n",
            "cleaning now\n",
            "getting_top_words\n",
            "70\n",
            "cleaning now\n",
            "getting_top_words\n",
            "71\n",
            "cleaning now\n",
            "getting_top_words\n",
            "72\n",
            "cleaning now\n",
            "getting_top_words\n",
            "73\n",
            "cleaning now\n",
            "getting_top_words\n",
            "74\n",
            "cleaning now\n",
            "getting_top_words\n",
            "75\n",
            "cleaning now\n",
            "getting_top_words\n",
            "76\n",
            "cleaning now\n",
            "getting_top_words\n",
            "77\n",
            "cleaning now\n",
            "getting_top_words\n",
            "78\n",
            "cleaning now\n",
            "getting_top_words\n",
            "79\n",
            "cleaning now\n",
            "getting_top_words\n",
            "80\n",
            "cleaning now\n",
            "getting_top_words\n",
            "81\n",
            "cleaning now\n",
            "getting_top_words\n",
            "82\n",
            "cleaning now\n",
            "getting_top_words\n",
            "83\n",
            "cleaning now\n",
            "getting_top_words\n",
            "84\n",
            "cleaning now\n",
            "getting_top_words\n",
            "85\n",
            "cleaning now\n",
            "getting_top_words\n",
            "86\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "86\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "86\n",
            "cleaning now\n",
            "getting_top_words\n",
            "87\n",
            "cleaning now\n",
            "getting_top_words\n",
            "88\n",
            "cleaning now\n",
            "getting_top_words\n",
            "89\n",
            "cleaning now\n",
            "getting_top_words\n",
            "90\n",
            "cleaning now\n",
            "getting_top_words\n",
            "91\n",
            "cleaning now\n",
            "getting_top_words\n",
            "92\n",
            "cleaning now\n",
            "getting_top_words\n",
            "93\n",
            "cleaning now\n",
            "getting_top_words\n",
            "94\n",
            "cleaning now\n",
            "getting_top_words\n",
            "95\n",
            "cleaning now\n",
            "getting_top_words\n",
            "96\n",
            "cleaning now\n",
            "getting_top_words\n",
            "97\n",
            "cleaning now\n",
            "getting_top_words\n",
            "98\n",
            "cleaning now\n",
            "getting_top_words\n",
            "99\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "99\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "99\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "99\n",
            "cleaning now\n",
            "getting_top_words\n",
            "100\n",
            "cleaning now\n",
            "getting_top_words\n",
            "101\n",
            "cleaning now\n",
            "getting_top_words\n",
            "102\n",
            "cleaning now\n",
            "getting_top_words\n",
            "103\n",
            "excepted\n",
            "103\n",
            "cleaning now\n",
            "getting_top_words\n",
            "104\n",
            "cleaning now\n",
            "getting_top_words\n",
            "105\n",
            "cleaning now\n",
            "getting_top_words\n",
            "106\n",
            "cleaning now\n",
            "getting_top_words\n",
            "107\n",
            "cleaning now\n",
            "getting_top_words\n",
            "108\n",
            "cleaning now\n",
            "getting_top_words\n",
            "109\n",
            "cleaning now\n",
            "getting_top_words\n",
            "110\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "110\n",
            "cleaning now\n",
            "getting_top_words\n",
            "111\n",
            "cleaning now\n",
            "getting_top_words\n",
            "112\n",
            "cleaning now\n",
            "getting_top_words\n",
            "113\n",
            "excepted\n",
            "113\n",
            "cleaning now\n",
            "getting_top_words\n",
            "114\n",
            "cleaning now\n",
            "getting_top_words\n",
            "115\n",
            "cleaning now\n",
            "getting_top_words\n",
            "116\n",
            "cleaning now\n",
            "getting_top_words\n",
            "117\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "117\n",
            "cleaning now\n",
            "getting_top_words\n",
            "118\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "118\n",
            "excepted\n",
            "118\n",
            "cleaning now\n",
            "getting_top_words\n",
            "119\n",
            "cleaning now\n",
            "getting_top_words\n",
            "120\n",
            "cleaning now\n",
            "getting_top_words\n",
            "121\n",
            "cleaning now\n",
            "getting_top_words\n",
            "122\n",
            "excepted\n",
            "122\n",
            "cleaning now\n",
            "getting_top_words\n",
            "123\n",
            "cleaning now\n",
            "getting_top_words\n",
            "124\n",
            "cleaning now\n",
            "getting_top_words\n",
            "125\n",
            "cleaning now\n",
            "getting_top_words\n",
            "126\n",
            "cleaning now\n",
            "getting_top_words\n",
            "127\n",
            "cleaning now\n",
            "getting_top_words\n",
            "128\n",
            "cleaning now\n",
            "getting_top_words\n",
            "129\n",
            "cleaning now\n",
            "getting_top_words\n",
            "130\n",
            "excepted\n",
            "130\n",
            "cleaning now\n",
            "getting_top_words\n",
            "131\n",
            "excepted\n",
            "131\n",
            "cleaning now\n",
            "getting_top_words\n",
            "132\n",
            "cleaning now\n",
            "getting_top_words\n",
            "133\n",
            "excepted\n",
            "133\n",
            "cleaning now\n",
            "getting_top_words\n",
            "134\n",
            "cleaning now\n",
            "getting_top_words\n",
            "135\n",
            "cleaning now\n",
            "getting_top_words\n",
            "136\n",
            "cleaning now\n",
            "getting_top_words\n",
            "137\n",
            "cleaning now\n",
            "getting_top_words\n",
            "138\n",
            "cleaning now\n",
            "getting_top_words\n",
            "139\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "139\n",
            "cleaning now\n",
            "getting_top_words\n",
            "140\n",
            "cleaning now\n",
            "getting_top_words\n",
            "141\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "141\n",
            "cleaning now\n",
            "getting_top_words\n",
            "142\n",
            "cleaning now\n",
            "getting_top_words\n",
            "143\n",
            "cleaning now\n",
            "getting_top_words\n",
            "144\n",
            "cleaning now\n",
            "getting_top_words\n",
            "145\n",
            "excepted\n",
            "145\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "145\n",
            "excepted\n",
            "145\n",
            "cleaning now\n",
            "getting_top_words\n",
            "146\n",
            "cleaning now\n",
            "getting_top_words\n",
            "147\n",
            "cleaning now\n",
            "getting_top_words\n",
            "148\n",
            "cleaning now\n",
            "getting_top_words\n",
            "149\n",
            "cleaning now\n",
            "getting_top_words\n",
            "150\n",
            "cleaning now\n",
            "getting_top_words\n",
            "151\n",
            "cleaning now\n",
            "getting_top_words\n",
            "152\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "152\n",
            "cleaning now\n",
            "getting_top_words\n",
            "153\n",
            "cleaning now\n",
            "getting_top_words\n",
            "154\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "154\n",
            "cleaning now\n",
            "getting_top_words\n",
            "155\n",
            "cleaning now\n",
            "getting_top_words\n",
            "156\n",
            "cleaning now\n",
            "getting_top_words\n",
            "157\n",
            "cleaning now\n",
            "getting_top_words\n",
            "158\n",
            "cleaning now\n",
            "getting_top_words\n",
            "159\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "159\n",
            "cleaning now\n",
            "getting_top_words\n",
            "160\n",
            "cleaning now\n",
            "getting_top_words\n",
            "161\n",
            "cleaning now\n",
            "getting_top_words\n",
            "162\n",
            "cleaning now\n",
            "getting_top_words\n",
            "163\n",
            "excepted\n",
            "163\n",
            "cleaning now\n",
            "getting_top_words\n",
            "164\n",
            "cleaning now\n",
            "getting_top_words\n",
            "165\n",
            "cleaning now\n",
            "getting_top_words\n",
            "166\n",
            "excepted\n",
            "166\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "166\n",
            "cleaning now\n",
            "getting_top_words\n",
            "167\n",
            "cleaning now\n",
            "getting_top_words\n",
            "168\n",
            "cleaning now\n",
            "getting_top_words\n",
            "169\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "169\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "169\n",
            "cleaning now\n",
            "getting_top_words\n",
            "170\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "170\n",
            "cleaning now\n",
            "getting_top_words\n",
            "171\n",
            "cleaning now\n",
            "getting_top_words\n",
            "172\n",
            "cleaning now\n",
            "getting_top_words\n",
            "173\n",
            "cleaning now\n",
            "getting_top_words\n",
            "174\n",
            "excepted\n",
            "174\n",
            "cleaning now\n",
            "getting_top_words\n",
            "175\n",
            "cleaning now\n",
            "getting_top_words\n",
            "176\n",
            "cleaning now\n",
            "getting_top_words\n",
            "177\n",
            "cleaning now\n",
            "getting_top_words\n",
            "178\n",
            "cleaning now\n",
            "getting_top_words\n",
            "179\n",
            "cleaning now\n",
            "getting_top_words\n",
            "180\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "180\n",
            "cleaning now\n",
            "getting_top_words\n",
            "181\n",
            "cleaning now\n",
            "getting_top_words\n",
            "182\n",
            "cleaning now\n",
            "getting_top_words\n",
            "183\n",
            "cleaning now\n",
            "getting_top_words\n",
            "184\n",
            "cleaning now\n",
            "getting_top_words\n",
            "185\n",
            "cleaning now\n",
            "getting_top_words\n",
            "186\n",
            "cleaning now\n",
            "getting_top_words\n",
            "187\n",
            "cleaning now\n",
            "getting_top_words\n",
            "188\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "188\n",
            "cleaning now\n",
            "getting_top_words\n",
            "189\n",
            "cleaning now\n",
            "getting_top_words\n",
            "190\n",
            "cleaning now\n",
            "getting_top_words\n",
            "191\n",
            "cleaning now\n",
            "getting_top_words\n",
            "192\n",
            "cleaning now\n",
            "getting_top_words\n",
            "193\n",
            "cleaning now\n",
            "getting_top_words\n",
            "194\n",
            "cleaning now\n",
            "getting_top_words\n",
            "195\n",
            "cleaning now\n",
            "getting_top_words\n",
            "196\n",
            "cleaning now\n",
            "getting_top_words\n",
            "197\n",
            "cleaning now\n",
            "getting_top_words\n",
            "198\n",
            "cleaning now\n",
            "getting_top_words\n",
            "199\n",
            "cleaning now\n",
            "getting_top_words\n",
            "200\n",
            "cleaning now\n",
            "getting_top_words\n",
            "201\n",
            "cleaning now\n",
            "getting_top_words\n",
            "202\n",
            "cleaning now\n",
            "getting_top_words\n",
            "203\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "203\n",
            "cleaning now\n",
            "getting_top_words\n",
            "204\n",
            "cleaning now\n",
            "getting_top_words\n",
            "205\n",
            "cleaning now\n",
            "getting_top_words\n",
            "206\n",
            "cleaning now\n",
            "getting_top_words\n",
            "207\n",
            "cleaning now\n",
            "getting_top_words\n",
            "208\n",
            "cleaning now\n",
            "getting_top_words\n",
            "209\n",
            "cleaning now\n",
            "getting_top_words\n",
            "210\n",
            "cleaning now\n",
            "getting_top_words\n",
            "211\n",
            "cleaning now\n",
            "getting_top_words\n",
            "212\n",
            "cleaning now\n",
            "getting_top_words\n",
            "213\n",
            "cleaning now\n",
            "getting_top_words\n",
            "214\n",
            "cleaning now\n",
            "getting_top_words\n",
            "215\n",
            "cleaning now\n",
            "getting_top_words\n",
            "216\n",
            "cleaning now\n",
            "getting_top_words\n",
            "217\n",
            "cleaning now\n",
            "getting_top_words\n",
            "218\n",
            "cleaning now\n",
            "getting_top_words\n",
            "219\n",
            "cleaning now\n",
            "getting_top_words\n",
            "220\n",
            "cleaning now\n",
            "getting_top_words\n",
            "221\n",
            "cleaning now\n",
            "getting_top_words\n",
            "222\n",
            "cleaning now\n",
            "getting_top_words\n",
            "223\n",
            "cleaning now\n",
            "getting_top_words\n",
            "224\n",
            "cleaning now\n",
            "getting_top_words\n",
            "225\n",
            "cleaning now\n",
            "getting_top_words\n",
            "226\n",
            "cleaning now\n",
            "getting_top_words\n",
            "227\n",
            "cleaning now\n",
            "getting_top_words\n",
            "228\n",
            "cleaning now\n",
            "getting_top_words\n",
            "229\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "229\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "229\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "229\n",
            "cleaning now\n",
            "getting_top_words\n",
            "230\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "230\n",
            "cleaning now\n",
            "getting_top_words\n",
            "231\n",
            "cleaning now\n",
            "getting_top_words\n",
            "232\n",
            "cleaning now\n",
            "getting_top_words\n",
            "233\n",
            "cleaning now\n",
            "getting_top_words\n",
            "234\n",
            "cleaning now\n",
            "getting_top_words\n",
            "235\n",
            "cleaning now\n",
            "getting_top_words\n",
            "236\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "236\n",
            "cleaning now\n",
            "getting_top_words\n",
            "237\n",
            "cleaning now\n",
            "getting_top_words\n",
            "238\n",
            "cleaning now\n",
            "getting_top_words\n",
            "239\n",
            "cleaning now\n",
            "getting_top_words\n",
            "240\n",
            "cleaning now\n",
            "getting_top_words\n",
            "241\n",
            "cleaning now\n",
            "getting_top_words\n",
            "242\n",
            "cleaning now\n",
            "getting_top_words\n",
            "243\n",
            "cleaning now\n",
            "getting_top_words\n",
            "244\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "244\n",
            "cleaning now\n",
            "getting_top_words\n",
            "245\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "245\n",
            "cleaning now\n",
            "getting_top_words\n",
            "246\n",
            "cleaning now\n",
            "getting_top_words\n",
            "247\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "247\n",
            "cleaning now\n",
            "getting_top_words\n",
            "248\n",
            "cleaning now\n",
            "getting_top_words\n",
            "249\n",
            "cleaning now\n",
            "getting_top_words\n",
            "250\n",
            "cleaning now\n",
            "getting_top_words\n",
            "251\n",
            "cleaning now\n",
            "getting_top_words\n",
            "252\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "252\n",
            "cleaning now\n",
            "getting_top_words\n",
            "253\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "253\n",
            "cleaning now\n",
            "getting_top_words\n",
            "254\n",
            "cleaning now\n",
            "getting_top_words\n",
            "255\n",
            "cleaning now\n",
            "getting_top_words\n",
            "256\n",
            "cleaning now\n",
            "getting_top_words\n",
            "257\n",
            "cleaning now\n",
            "getting_top_words\n",
            "258\n",
            "cleaning now\n",
            "getting_top_words\n",
            "259\n",
            "cleaning now\n",
            "getting_top_words\n",
            "260\n",
            "cleaning now\n",
            "getting_top_words\n",
            "261\n",
            "cleaning now\n",
            "getting_top_words\n",
            "262\n",
            "cleaning now\n",
            "getting_top_words\n",
            "263\n",
            "cleaning now\n",
            "getting_top_words\n",
            "264\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "264\n",
            "cleaning now\n",
            "getting_top_words\n",
            "265\n",
            "cleaning now\n",
            "getting_top_words\n",
            "266\n",
            "cleaning now\n",
            "getting_top_words\n",
            "267\n",
            "cleaning now\n",
            "getting_top_words\n",
            "268\n",
            "cleaning now\n",
            "getting_top_words\n",
            "269\n",
            "cleaning now\n",
            "getting_top_words\n",
            "270\n",
            "cleaning now\n",
            "getting_top_words\n",
            "271\n",
            "cleaning now\n",
            "getting_top_words\n",
            "272\n",
            "cleaning now\n",
            "getting_top_words\n",
            "273\n",
            "cleaning now\n",
            "getting_top_words\n",
            "274\n",
            "cleaning now\n",
            "getting_top_words\n",
            "275\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "275\n",
            "cleaning now\n",
            "getting_top_words\n",
            "276\n",
            "cleaning now\n",
            "getting_top_words\n",
            "277\n",
            "cleaning now\n",
            "getting_top_words\n",
            "278\n",
            "cleaning now\n",
            "getting_top_words\n",
            "279\n",
            "cleaning now\n",
            "getting_top_words\n",
            "280\n",
            "cleaning now\n",
            "getting_top_words\n",
            "281\n",
            "cleaning now\n",
            "getting_top_words\n",
            "282\n",
            "cleaning now\n",
            "getting_top_words\n",
            "283\n",
            "cleaning now\n",
            "getting_top_words\n",
            "284\n",
            "cleaning now\n",
            "getting_top_words\n",
            "285\n",
            "cleaning now\n",
            "getting_top_words\n",
            "286\n",
            "cleaning now\n",
            "getting_top_words\n",
            "287\n",
            "cleaning now\n",
            "getting_top_words\n",
            "288\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "288\n",
            "cleaning now\n",
            "getting_top_words\n",
            "289\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "289\n",
            "cleaning now\n",
            "getting_top_words\n",
            "290\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "290\n",
            "cleaning now\n",
            "getting_top_words\n",
            "291\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "291\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "291\n",
            "cleaning now\n",
            "getting_top_words\n",
            "292\n",
            "cleaning now\n",
            "getting_top_words\n",
            "293\n",
            "cleaning now\n",
            "getting_top_words\n",
            "294\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "294\n",
            "cleaning now\n",
            "getting_top_words\n",
            "295\n",
            "cleaning now\n",
            "getting_top_words\n",
            "296\n",
            "cleaning now\n",
            "getting_top_words\n",
            "297\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "297\n",
            "cleaning now\n",
            "getting_top_words\n",
            "298\n",
            "cleaning now\n",
            "getting_top_words\n",
            "299\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "299\n",
            "cleaning now\n",
            "getting_top_words\n",
            "300\n",
            "cleaning now\n",
            "getting_top_words\n",
            "301\n",
            "cleaning now\n",
            "getting_top_words\n",
            "302\n",
            "cleaning now\n",
            "getting_top_words\n",
            "303\n",
            "cleaning now\n",
            "getting_top_words\n",
            "304\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "304\n",
            "excepted\n",
            "304\n",
            "excepted\n",
            "304\n",
            "cleaning now\n",
            "getting_top_words\n",
            "305\n",
            "cleaning now\n",
            "getting_top_words\n",
            "306\n",
            "cleaning now\n",
            "getting_top_words\n",
            "307\n",
            "cleaning now\n",
            "getting_top_words\n",
            "308\n",
            "cleaning now\n",
            "getting_top_words\n",
            "309\n",
            "cleaning now\n",
            "getting_top_words\n",
            "310\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "310\n",
            "cleaning now\n",
            "getting_top_words\n",
            "311\n",
            "cleaning now\n",
            "getting_top_words\n",
            "312\n",
            "cleaning now\n",
            "getting_top_words\n",
            "313\n",
            "cleaning now\n",
            "getting_top_words\n",
            "314\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "314\n",
            "cleaning now\n",
            "getting_top_words\n",
            "315\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "315\n",
            "excepted\n",
            "315\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "315\n",
            "cleaning now\n",
            "getting_top_words\n",
            "316\n",
            "cleaning now\n",
            "getting_top_words\n",
            "317\n",
            "cleaning now\n",
            "getting_top_words\n",
            "318\n",
            "cleaning now\n",
            "getting_top_words\n",
            "319\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "319\n",
            "cleaning now\n",
            "getting_top_words\n",
            "320\n",
            "excepted\n",
            "320\n",
            "cleaning now\n",
            "getting_top_words\n",
            "321\n",
            "cleaning now\n",
            "getting_top_words\n",
            "322\n",
            "excepted\n",
            "322\n",
            "cleaning now\n",
            "getting_top_words\n",
            "323\n",
            "cleaning now\n",
            "getting_top_words\n",
            "324\n",
            "cleaning now\n",
            "getting_top_words\n",
            "325\n",
            "cleaning now\n",
            "getting_top_words\n",
            "326\n",
            "cleaning now\n",
            "getting_top_words\n",
            "327\n",
            "excepted\n",
            "327\n",
            "cleaning now\n",
            "getting_top_words\n",
            "328\n",
            "excepted\n",
            "328\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "328\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "328\n",
            "cleaning now\n",
            "getting_top_words\n",
            "329\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "329\n",
            "cleaning now\n",
            "getting_top_words\n",
            "330\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "330\n",
            "cleaning now\n",
            "getting_top_words\n",
            "331\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "331\n",
            "excepted\n",
            "331\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "331\n",
            "excepted\n",
            "331\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "331\n",
            "cleaning now\n",
            "getting_top_words\n",
            "332\n",
            "cleaning now\n",
            "getting_top_words\n",
            "333\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "333\n",
            "excepted\n",
            "333\n",
            "cleaning now\n",
            "getting_top_words\n",
            "334\n",
            "cleaning now\n",
            "getting_top_words\n",
            "335\n",
            "excepted\n",
            "335\n",
            "cleaning now\n",
            "getting_top_words\n",
            "336\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "336\n",
            "cleaning now\n",
            "getting_top_words\n",
            "337\n",
            "cleaning now\n",
            "getting_top_words\n",
            "338\n",
            "cleaning now\n",
            "getting_top_words\n",
            "339\n",
            "excepted\n",
            "339\n",
            "cleaning now\n",
            "getting_top_words\n",
            "340\n",
            "cleaning now\n",
            "getting_top_words\n",
            "341\n",
            "excepted\n",
            "341\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "341\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "341\n",
            "cleaning now\n",
            "getting_top_words\n",
            "342\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "342\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "342\n",
            "cleaning now\n",
            "getting_top_words\n",
            "343\n",
            "excepted\n",
            "343\n",
            "cleaning now\n",
            "getting_top_words\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "excepted\n",
            "344\n",
            "excepted\n",
            "344\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "excepted\n",
            "344\n",
            "excepted\n",
            "344\n",
            "cleaning now\n",
            "getting_top_words\n",
            "345\n",
            "cleaning now\n",
            "getting_top_words\n",
            "346\n",
            "cleaning now\n",
            "getting_top_words\n",
            "347\n",
            "cleaning now\n",
            "getting_top_words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLUsDd5bhG17",
        "colab_type": "code",
        "outputId": "cc3b7c0b-b7fa-41b2-fa25-e9a56ab25e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "final_vectors.to_csv(\"final_top_words_model-1.csv\")\n",
        "files.download(\"final_top_words_model-1.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-05e55a662998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_top_words_model-1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_top_words_model-1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HECNZhS9o0Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"final_top_words_model-1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn8CdDYihV0y",
        "colab_type": "text"
      },
      "source": [
        "## Creating X matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEYoinrhjUo",
        "colab_type": "code",
        "outputId": "c691db68-8683-4a92-d488-ed5303097fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "for_tensor = final_vectors.merge(parent_list, left_on=\"videoId\", right_on=\"uni_code\")\n",
        "for_tensor.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>videoId</th>\n",
              "      <th>top_words</th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>lower_thres</th>\n",
              "      <th>Week</th>\n",
              "      <th>Cast 1</th>\n",
              "      <th>Cast 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>[kon, mein, musical, must, na, nail, ne, new, ...</td>\n",
              "      <td>October</td>\n",
              "      <td>05</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte,</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>2018-09-14</td>\n",
              "      <td>40</td>\n",
              "      <td>ayushman khurana</td>\n",
              "      <td>tabu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>[zero, least, la, lady, lake, train, late, tot...</td>\n",
              "      <td>March</td>\n",
              "      <td>08</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu,</td>\n",
              "      <td>Red Chillies Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>2019-02-15</td>\n",
              "      <td>10</td>\n",
              "      <td>amitabh bachchan</td>\n",
              "      <td>taapsee pannu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>[smelly, instigate, interrogate, internal, int...</td>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>2018-12-21</td>\n",
              "      <td>2</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>mohit raina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>[able, normal, par, palsy, palm, pa, outstandi...</td>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji,</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16</td>\n",
              "      <td>2018-03-02</td>\n",
              "      <td>12</td>\n",
              "      <td>rani mukerji</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>[latest, horn, id, ich, hygiene, hundred, huma...</td>\n",
              "      <td>February</td>\n",
              "      <td>09</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte,</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>2018-01-19</td>\n",
              "      <td>6</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>sonam kapoor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       videoId  ...         Cast 2\n",
              "0  2iVYI99VGaw  ...           tabu\n",
              "1  mSlgu8AQAd4  ...  taapsee pannu\n",
              "2  VVY3do673Zc  ...    mohit raina\n",
              "3  nLSaCFlXn-g  ...               \n",
              "4  -K9ujx8vO_A  ...   sonam kapoor\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deyrn-jKh5ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list=list(for_tensor['top_words'])\n",
        "words_set=set([item for sublist in final_list for item in sublist])\n",
        "\n",
        "\n",
        "#final_cast\n",
        "#for_tensor['Cast']=[x[0:1] for x in for_tensor['Cast']]\n",
        "final_cast=list(for_tensor['Cast 1'])\n",
        "final_cast=final_cast+list(for_tensor['Cast 2'])\n",
        "cast_set=set(final_cast)\n",
        "\n",
        "\n",
        "#director\n",
        "final_dir=list(for_tensor['Director'])\n",
        "#dir_set=set([item for sublist in final_dir for item in sublist])\n",
        "\n",
        "\n",
        "#production house\n",
        "#for_tensor['Studio']=[x[0:1] for x in for_tensor['Studio']]\n",
        "final_pro=list(for_tensor['Studio'])\n",
        "#pro_set=set([item for sublist in final_pro for item in sublist])\n",
        "\n",
        "words_set.update(cast_set)\n",
        "words_set.update(final_pro)\n",
        "words_set.update(final_dir)\n",
        "\n",
        "words_set=list(words_set)\n",
        "\n",
        "word_ids=list(range(0,len(words_set)))\n",
        "\n",
        "dict_cov=dict(zip(words_set,word_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6iMG4mSo9yX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fICMOgeiOA8",
        "colab_type": "code",
        "outputId": "d9a2362a-859d-4361-b066-b6e324647933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rows=[]\n",
        "columns=[]\n",
        "data=[]\n",
        "for i in range(0,len(for_tensor)):\n",
        "  print(i)\n",
        "  words=for_tensor.iloc[i]['top_words']\n",
        "  words=[dict_cov[x] for x in words]\n",
        "  columns=columns+words\n",
        "\n",
        "  cast=[for_tensor.iloc[i]['Cast 1'],for_tensor.iloc[i]['Cast 2']]\n",
        "  cast=[dict_cov[x] for x in cast]\n",
        "  columns=columns+cast\n",
        "  \n",
        "  dire=[for_tensor.iloc[i]['Director']]\n",
        "  dire=[dict_cov[x] for x in dire]\n",
        "  columns=columns+dire\n",
        "\n",
        "  pro=[for_tensor.iloc[i]['Studio']]\n",
        "  pro=[dict_cov[x] for x in pro]\n",
        "  columns=columns+pro\n",
        "\n",
        "  n=len(dire)+len(cast)+len(pro)+len(words)\n",
        "  row=[i]*n\n",
        "  vals=[1]*n\n",
        "\n",
        "  rows=rows+row\n",
        "  data=data+vals\n",
        "\n",
        "mat=sparse.coo_matrix((data,(rows,columns)),shape=(len(for_tensor),len(words_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ruVh1Si161",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_t=mat.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ04JbPYi90v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_result1 = np.array(for_tensor['Subcategories 1'])\n",
        "cat_result2 = np.array(for_tensor['Subcategories 2'])\n",
        "cat_result3 = np.array(for_tensor['Subcategories 3'])\n",
        "#org_mat = mat.toarray()\n",
        "org_mat_df = pd.DataFrame(data=s_t, columns=['c'+str(i) for i in range(s_t.shape[1])])\n",
        "org_mat_df.to_csv(\"/content/drive/My Drive/project/Results/model1_data_200.csv\")\n",
        "cat_result_df1 = pd.DataFrame({'t': cat_result1})\n",
        "cat_result_df1.to_csv(\"/content/drive/My Drive/project/Results/model1_data_target_1.csv\")\n",
        "cat_result_df2 = pd.DataFrame({'t': cat_result2})\n",
        "cat_result_df2.to_csv(\"/content/drive/My Drive/project/Results/model1_data_target_2.csv\")\n",
        "cat_result_df3 = pd.DataFrame({'t': cat_result3})\n",
        "cat_result_df3.to_csv(\"/content/drive/My Drive/project/Results/model1_data_target_3.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}