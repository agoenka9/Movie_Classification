{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yJHC1Dmpw98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime,timedelta\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI5qd5dap6UL",
        "colab_type": "code",
        "outputId": "59d38ed1-cfb6-4ffa-c20c-03ebbf08c833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-L4lDU8qDSG",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv5lNQ4bqCYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_excel(\"/content/Movie_dataset_3subcategories.xlsx\",sheet_name=\"All years combined\")\n",
        "#df_2019=df[df['Year']==2019][['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories']]\n",
        "df_2019=df[['Year','Opening','Date','Title','Unnamed: 4','Director','Cast','Studio (production house)','Subcategories 1','Subcategories 2','Subcategories 3']]\n",
        "df_2019=df_2019.rename(columns={\"Unnamed: 4\":\"uni_code\",\"Studio (production house)\":\"Studio\"})\n",
        "df_2019=df_2019[df_2019['uni_code']!=\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmgVeGl7qJsS",
        "colab_type": "code",
        "outputId": "42e02dda-b169-437d-d374-9b53eada92fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "df_2019.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018</td>\n",
              "      <td>October</td>\n",
              "      <td>5</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019</td>\n",
              "      <td>March</td>\n",
              "      <td>8</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu</td>\n",
              "      <td>Red Chillies Entertainment, Azure Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>February</td>\n",
              "      <td>9</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment, Hope P...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year   Opening  Date  ... Subcategories 1 Subcategories 2 Subcategories 3\n",
              "0  2018   October     5  ...               4               4               3\n",
              "1  2019     March     8  ...               4               4               3\n",
              "2  2019   January    11  ...               4               4               3\n",
              "3  2018     March    23  ...               4               4               3\n",
              "4  2018  February     9  ...               4               4               3\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zDRY5AIqKZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the threshold datettime\n",
        "\n",
        "df_2019['Month']=[datetime.strptime(x,\"%B\").month for x in df_2019['Opening']]\n",
        "\n",
        "df_2019['Month']=df_2019['Month'].astype(str)\n",
        "df_2019['Month']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Month']]\n",
        "df_2019['Date']=df_2019['Date'].astype(str)\n",
        "df_2019['Date']=[\"0\"+ x if len(x)==1 else x for x in df_2019['Date']]\n",
        "\n",
        "df_2019['Datef']=df_2019['Year'].astype(str)+\"-\"+df_2019['Month'].astype(str)+\"-\"+df_2019['Date'].astype(str)\n",
        "\n",
        "df_2019['Datef']=[datetime.strptime(x,\"%Y-%m-%d\") for x in df_2019['Datef']]\n",
        "df_2019['Threshold']=[x-timedelta(days=7) for x in df_2019['Datef']]\n",
        "df_2019['lower_thres']=[x-timedelta(days=21) for x in df_2019['Datef']]\n",
        "df_2019['Week']=[x.isocalendar()[1] for x in df_2019['Datef']]\n",
        "del df_2019['Datef']\n",
        "del df_2019['Month']\n",
        "del df_2019['Year']\n",
        "\n",
        "df_2019=df_2019.fillna(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgxk1OX0qMYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parsing cast,director,studio to list\n",
        "df_2019['Cast']=[x+\", \" for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.split(\",\")[0] for x in df_2019['Cast']]\n",
        "df_2019['Cast 1']=[x.lower() for x in df_2019['Cast 1']]\n",
        "df_2019['Cast 1']=[x.strip() for x in df_2019['Cast 1']]\n",
        "\n",
        "df_2019['Cast 2']=[x.split(\",\")[1] for x in df_2019['Cast']]\n",
        "df_2019['Cast 2']=[x.lower() for x in df_2019['Cast 2']]\n",
        "df_2019['Cast 2']=[x.strip() for x in df_2019['Cast 2']]\n",
        "\n",
        "df_2019['Director']=[x.split(\",\")[0] for x in df_2019['Director']]\n",
        "df_2019['Studio']=[x.split(\",\")[0] for x in df_2019['Studio']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aFfGtu0qOMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_2019.to_csv(\"all_celeaned_model-2.csv\")\n",
        "files.download(\"all_celeaned_model-2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OopG8ZH8qQzu",
        "colab_type": "text"
      },
      "source": [
        "## Developing Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwY8Wf2qUaZ",
        "colab_type": "code",
        "outputId": "856c428f-ece2-4274-b2de-5f7b69eb6c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#installing required packages\n",
        "pip install emot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfx62HopqWRd",
        "colab_type": "code",
        "outputId": "6303a167-4a37-4a75-900a-22dd533d1ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#importing required libraries\n",
        "import nltk.data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import words\n",
        "from itertools import combinations\n",
        "import random as rd\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "#from googletrans import Translator\n",
        "from datetime import datetime\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import ast\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpaAyYxNqYZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parent_list=df_2019.copy()\n",
        "L_zip =list(zip(parent_list['uni_code'],parent_list['Threshold'],parent_list['lower_thres']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIAdUyrxqbGU",
        "colab_type": "text"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKUugoK8qaNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Declaring all necessary functions\n",
        "setofwords = set(words.words())\n",
        "\n",
        "#Convert emojis to text\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \" \" + \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "        #text = text.replace(emot,\"\")\n",
        "    return text\n",
        "\n",
        "#apply emoji convert & translate hindi to english\n",
        "def convert_lang(x,date1,date2): \n",
        "  path=\"/content/drive/My Drive/project/allmovies/\"+x+\".csv\"\n",
        "  try:\n",
        "    df=pd.read_csv(path,engine='python')\n",
        "  except:\n",
        "    df=pd.read_csv(path,engine='c')\n",
        "  del df['Unnamed: 0']\n",
        "  df=df.dropna()\n",
        "  df['pub_dates']=[datetime.strptime(x.split(\"T\")[0], \"%Y-%m-%d\") for x in df['pub_dates']]\n",
        "  comments=df[(df['pub_dates']<date1) & (df['pub_dates']>date2)]['comments']\n",
        "  comments=[convert_emojis(x) for x in comments]\n",
        "  return comments\n",
        "\n",
        "#remove punctuations, special characters and stop words from the text\n",
        "def clean_comments(comment):\n",
        "  # removing all non aplhabetic characters\n",
        "  clean_comment=re.sub(\"[^a-zA-Z]+\",\" \",comment)\n",
        "\n",
        "  #lowercase & split\n",
        "  clean_comment = clean_comment.lower()\n",
        "  clean_comment = clean_comment.split()\n",
        "\n",
        "  #remove stop words & join\n",
        "  from nltk.corpus import stopwords\n",
        "  clean_comment = [w for w in clean_comment if not w in stopwords.words(\"english\")]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  clean_comment=[lemmatizer.lemmatize(w) for w in clean_comment]\n",
        "  clean_comment=[x for x in clean_comment if x in setofwords]\n",
        "  clean_comment=\" \".join( clean_comment)\n",
        "\n",
        "  return clean_comment\n",
        "\n",
        "#get words that have highest tfidf score\n",
        "def get_topwords(cleaned_comments):\n",
        "  print(\"getting_top_words\")\n",
        "  cv=CountVectorizer(max_features=5000)\n",
        "  word_count_vector=cv.fit_transform(cleaned_comments)\n",
        "  tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "  tfidf_transformer.fit(word_count_vector)\n",
        "  df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
        "  df_idf=df_idf.sort_values(by=[\"idf_weights\"],ascending=False).iloc[0:200]\n",
        "  return list(df_idf.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2V08TPmqik1",
        "colab_type": "code",
        "outputId": "d2259b27-b100-4d80-ca91-ae224ed54c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_vectors = pd.DataFrame(columns=['videoId','top_words'])\n",
        "L_ex=[]\n",
        "i=0\n",
        "cleaned_comm=[]\n",
        "for x,date1,date2 in L_zip:\n",
        "  print(i)\n",
        "  try:\n",
        "    proc_comments = convert_lang(x,date1,date2)\n",
        "    print(\"cleaning now\")\n",
        "    cleaned_comments=[clean_comments(comment) for comment in proc_comments]\n",
        "    cleaned_comm=cleaned_comm+[\".\".join(cleaned_comments)]\n",
        "    i=i+1\n",
        "  except:\n",
        "    print(\"excepted\")\n",
        "    print(i)\n",
        "    L_ex=L_ex+[i]\n",
        "    i=i+1\n",
        "    continue\n",
        "  \n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None, preprocessor = None, stop_words = None,max_features = 200)\n",
        "train_data_features = vectorizer.fit_transform(cleaned_comm)\n",
        "X_counts=train_data_features.toarray()\n",
        "transformer=TfidfTransformer(smooth_idf=False)\n",
        "tfidf=transformer.fit_transform(X_counts)\n",
        "X_tfidf=tfidf.toarray()\n",
        "X_tfidf2=np.where(X_tfidf>0,1,X_tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "cleaning now\n",
            "1\n",
            "cleaning now\n",
            "2\n",
            "cleaning now\n",
            "3\n",
            "cleaning now\n",
            "4\n",
            "cleaning now\n",
            "5\n",
            "cleaning now\n",
            "6\n",
            "cleaning now\n",
            "7\n",
            "cleaning now\n",
            "8\n",
            "cleaning now\n",
            "9\n",
            "cleaning now\n",
            "10\n",
            "cleaning now\n",
            "11\n",
            "cleaning now\n",
            "12\n",
            "cleaning now\n",
            "13\n",
            "cleaning now\n",
            "14\n",
            "cleaning now\n",
            "15\n",
            "cleaning now\n",
            "16\n",
            "cleaning now\n",
            "17\n",
            "cleaning now\n",
            "18\n",
            "cleaning now\n",
            "19\n",
            "cleaning now\n",
            "20\n",
            "cleaning now\n",
            "21\n",
            "cleaning now\n",
            "22\n",
            "cleaning now\n",
            "23\n",
            "cleaning now\n",
            "24\n",
            "cleaning now\n",
            "25\n",
            "cleaning now\n",
            "26\n",
            "cleaning now\n",
            "27\n",
            "cleaning now\n",
            "28\n",
            "cleaning now\n",
            "29\n",
            "cleaning now\n",
            "30\n",
            "cleaning now\n",
            "31\n",
            "cleaning now\n",
            "32\n",
            "cleaning now\n",
            "33\n",
            "cleaning now\n",
            "34\n",
            "cleaning now\n",
            "35\n",
            "cleaning now\n",
            "36\n",
            "cleaning now\n",
            "37\n",
            "cleaning now\n",
            "38\n",
            "cleaning now\n",
            "39\n",
            "cleaning now\n",
            "40\n",
            "cleaning now\n",
            "41\n",
            "cleaning now\n",
            "42\n",
            "cleaning now\n",
            "43\n",
            "cleaning now\n",
            "44\n",
            "cleaning now\n",
            "45\n",
            "cleaning now\n",
            "46\n",
            "cleaning now\n",
            "47\n",
            "cleaning now\n",
            "48\n",
            "cleaning now\n",
            "49\n",
            "cleaning now\n",
            "50\n",
            "cleaning now\n",
            "51\n",
            "cleaning now\n",
            "52\n",
            "cleaning now\n",
            "53\n",
            "cleaning now\n",
            "54\n",
            "cleaning now\n",
            "55\n",
            "cleaning now\n",
            "56\n",
            "cleaning now\n",
            "57\n",
            "cleaning now\n",
            "58\n",
            "cleaning now\n",
            "59\n",
            "cleaning now\n",
            "60\n",
            "cleaning now\n",
            "61\n",
            "cleaning now\n",
            "62\n",
            "cleaning now\n",
            "63\n",
            "cleaning now\n",
            "64\n",
            "cleaning now\n",
            "65\n",
            "cleaning now\n",
            "66\n",
            "cleaning now\n",
            "67\n",
            "cleaning now\n",
            "68\n",
            "cleaning now\n",
            "69\n",
            "cleaning now\n",
            "70\n",
            "cleaning now\n",
            "71\n",
            "cleaning now\n",
            "72\n",
            "cleaning now\n",
            "73\n",
            "cleaning now\n",
            "74\n",
            "cleaning now\n",
            "75\n",
            "cleaning now\n",
            "76\n",
            "cleaning now\n",
            "77\n",
            "cleaning now\n",
            "78\n",
            "cleaning now\n",
            "79\n",
            "cleaning now\n",
            "80\n",
            "cleaning now\n",
            "81\n",
            "cleaning now\n",
            "82\n",
            "cleaning now\n",
            "83\n",
            "cleaning now\n",
            "84\n",
            "cleaning now\n",
            "85\n",
            "cleaning now\n",
            "86\n",
            "cleaning now\n",
            "87\n",
            "cleaning now\n",
            "88\n",
            "cleaning now\n",
            "89\n",
            "cleaning now\n",
            "90\n",
            "cleaning now\n",
            "91\n",
            "cleaning now\n",
            "92\n",
            "cleaning now\n",
            "93\n",
            "cleaning now\n",
            "94\n",
            "cleaning now\n",
            "95\n",
            "cleaning now\n",
            "96\n",
            "cleaning now\n",
            "97\n",
            "cleaning now\n",
            "98\n",
            "cleaning now\n",
            "99\n",
            "cleaning now\n",
            "100\n",
            "cleaning now\n",
            "101\n",
            "cleaning now\n",
            "102\n",
            "cleaning now\n",
            "103\n",
            "cleaning now\n",
            "104\n",
            "cleaning now\n",
            "105\n",
            "cleaning now\n",
            "106\n",
            "cleaning now\n",
            "107\n",
            "cleaning now\n",
            "108\n",
            "cleaning now\n",
            "109\n",
            "cleaning now\n",
            "110\n",
            "cleaning now\n",
            "111\n",
            "cleaning now\n",
            "112\n",
            "cleaning now\n",
            "113\n",
            "excepted\n",
            "113\n",
            "114\n",
            "cleaning now\n",
            "115\n",
            "cleaning now\n",
            "116\n",
            "cleaning now\n",
            "117\n",
            "cleaning now\n",
            "118\n",
            "cleaning now\n",
            "119\n",
            "cleaning now\n",
            "120\n",
            "cleaning now\n",
            "121\n",
            "cleaning now\n",
            "122\n",
            "cleaning now\n",
            "123\n",
            "cleaning now\n",
            "124\n",
            "cleaning now\n",
            "125\n",
            "excepted\n",
            "125\n",
            "126\n",
            "cleaning now\n",
            "127\n",
            "cleaning now\n",
            "128\n",
            "cleaning now\n",
            "129\n",
            "cleaning now\n",
            "130\n",
            "cleaning now\n",
            "131\n",
            "cleaning now\n",
            "132\n",
            "cleaning now\n",
            "133\n",
            "excepted\n",
            "133\n",
            "134\n",
            "cleaning now\n",
            "135\n",
            "cleaning now\n",
            "136\n",
            "cleaning now\n",
            "137\n",
            "cleaning now\n",
            "138\n",
            "excepted\n",
            "138\n",
            "139\n",
            "cleaning now\n",
            "140\n",
            "cleaning now\n",
            "141\n",
            "cleaning now\n",
            "142\n",
            "cleaning now\n",
            "143\n",
            "cleaning now\n",
            "144\n",
            "cleaning now\n",
            "145\n",
            "cleaning now\n",
            "146\n",
            "cleaning now\n",
            "147\n",
            "excepted\n",
            "147\n",
            "148\n",
            "cleaning now\n",
            "149\n",
            "excepted\n",
            "149\n",
            "150\n",
            "cleaning now\n",
            "151\n",
            "cleaning now\n",
            "152\n",
            "excepted\n",
            "152\n",
            "153\n",
            "cleaning now\n",
            "154\n",
            "cleaning now\n",
            "155\n",
            "cleaning now\n",
            "156\n",
            "cleaning now\n",
            "157\n",
            "cleaning now\n",
            "158\n",
            "cleaning now\n",
            "159\n",
            "cleaning now\n",
            "160\n",
            "cleaning now\n",
            "161\n",
            "cleaning now\n",
            "162\n",
            "cleaning now\n",
            "163\n",
            "cleaning now\n",
            "164\n",
            "cleaning now\n",
            "165\n",
            "cleaning now\n",
            "166\n",
            "cleaning now\n",
            "167\n",
            "excepted\n",
            "167\n",
            "168\n",
            "cleaning now\n",
            "169\n",
            "excepted\n",
            "169\n",
            "170\n",
            "cleaning now\n",
            "171\n",
            "cleaning now\n",
            "172\n",
            "cleaning now\n",
            "173\n",
            "cleaning now\n",
            "174\n",
            "cleaning now\n",
            "175\n",
            "cleaning now\n",
            "176\n",
            "cleaning now\n",
            "177\n",
            "cleaning now\n",
            "178\n",
            "cleaning now\n",
            "179\n",
            "cleaning now\n",
            "180\n",
            "cleaning now\n",
            "181\n",
            "cleaning now\n",
            "182\n",
            "cleaning now\n",
            "183\n",
            "cleaning now\n",
            "184\n",
            "cleaning now\n",
            "185\n",
            "cleaning now\n",
            "186\n",
            "cleaning now\n",
            "187\n",
            "cleaning now\n",
            "188\n",
            "cleaning now\n",
            "189\n",
            "cleaning now\n",
            "190\n",
            "cleaning now\n",
            "191\n",
            "excepted\n",
            "191\n",
            "192\n",
            "cleaning now\n",
            "193\n",
            "cleaning now\n",
            "194\n",
            "cleaning now\n",
            "195\n",
            "excepted\n",
            "195\n",
            "196\n",
            "cleaning now\n",
            "197\n",
            "cleaning now\n",
            "198\n",
            "cleaning now\n",
            "199\n",
            "cleaning now\n",
            "200\n",
            "cleaning now\n",
            "201\n",
            "cleaning now\n",
            "202\n",
            "cleaning now\n",
            "203\n",
            "cleaning now\n",
            "204\n",
            "cleaning now\n",
            "205\n",
            "cleaning now\n",
            "206\n",
            "cleaning now\n",
            "207\n",
            "cleaning now\n",
            "208\n",
            "excepted\n",
            "208\n",
            "209\n",
            "cleaning now\n",
            "210\n",
            "cleaning now\n",
            "211\n",
            "cleaning now\n",
            "212\n",
            "cleaning now\n",
            "213\n",
            "cleaning now\n",
            "214\n",
            "cleaning now\n",
            "215\n",
            "cleaning now\n",
            "216\n",
            "cleaning now\n",
            "217\n",
            "cleaning now\n",
            "218\n",
            "cleaning now\n",
            "219\n",
            "cleaning now\n",
            "220\n",
            "cleaning now\n",
            "221\n",
            "cleaning now\n",
            "222\n",
            "cleaning now\n",
            "223\n",
            "cleaning now\n",
            "224\n",
            "cleaning now\n",
            "225\n",
            "cleaning now\n",
            "226\n",
            "cleaning now\n",
            "227\n",
            "cleaning now\n",
            "228\n",
            "cleaning now\n",
            "229\n",
            "cleaning now\n",
            "230\n",
            "cleaning now\n",
            "231\n",
            "cleaning now\n",
            "232\n",
            "cleaning now\n",
            "233\n",
            "cleaning now\n",
            "234\n",
            "cleaning now\n",
            "235\n",
            "cleaning now\n",
            "236\n",
            "cleaning now\n",
            "237\n",
            "cleaning now\n",
            "238\n",
            "cleaning now\n",
            "239\n",
            "cleaning now\n",
            "240\n",
            "cleaning now\n",
            "241\n",
            "cleaning now\n",
            "242\n",
            "cleaning now\n",
            "243\n",
            "cleaning now\n",
            "244\n",
            "cleaning now\n",
            "245\n",
            "cleaning now\n",
            "246\n",
            "cleaning now\n",
            "247\n",
            "cleaning now\n",
            "248\n",
            "cleaning now\n",
            "249\n",
            "cleaning now\n",
            "250\n",
            "cleaning now\n",
            "251\n",
            "cleaning now\n",
            "252\n",
            "cleaning now\n",
            "253\n",
            "cleaning now\n",
            "254\n",
            "cleaning now\n",
            "255\n",
            "cleaning now\n",
            "256\n",
            "cleaning now\n",
            "257\n",
            "cleaning now\n",
            "258\n",
            "cleaning now\n",
            "259\n",
            "cleaning now\n",
            "260\n",
            "cleaning now\n",
            "261\n",
            "cleaning now\n",
            "262\n",
            "cleaning now\n",
            "263\n",
            "cleaning now\n",
            "264\n",
            "cleaning now\n",
            "265\n",
            "cleaning now\n",
            "266\n",
            "cleaning now\n",
            "267\n",
            "cleaning now\n",
            "268\n",
            "cleaning now\n",
            "269\n",
            "cleaning now\n",
            "270\n",
            "cleaning now\n",
            "271\n",
            "cleaning now\n",
            "272\n",
            "cleaning now\n",
            "273\n",
            "cleaning now\n",
            "274\n",
            "cleaning now\n",
            "275\n",
            "cleaning now\n",
            "276\n",
            "cleaning now\n",
            "277\n",
            "cleaning now\n",
            "278\n",
            "cleaning now\n",
            "279\n",
            "cleaning now\n",
            "280\n",
            "cleaning now\n",
            "281\n",
            "cleaning now\n",
            "282\n",
            "cleaning now\n",
            "283\n",
            "cleaning now\n",
            "284\n",
            "cleaning now\n",
            "285\n",
            "cleaning now\n",
            "286\n",
            "cleaning now\n",
            "287\n",
            "cleaning now\n",
            "288\n",
            "cleaning now\n",
            "289\n",
            "cleaning now\n",
            "290\n",
            "cleaning now\n",
            "291\n",
            "cleaning now\n",
            "292\n",
            "cleaning now\n",
            "293\n",
            "cleaning now\n",
            "294\n",
            "cleaning now\n",
            "295\n",
            "cleaning now\n",
            "296\n",
            "cleaning now\n",
            "297\n",
            "cleaning now\n",
            "298\n",
            "cleaning now\n",
            "299\n",
            "cleaning now\n",
            "300\n",
            "cleaning now\n",
            "301\n",
            "cleaning now\n",
            "302\n",
            "cleaning now\n",
            "303\n",
            "cleaning now\n",
            "304\n",
            "cleaning now\n",
            "305\n",
            "cleaning now\n",
            "306\n",
            "cleaning now\n",
            "307\n",
            "cleaning now\n",
            "308\n",
            "cleaning now\n",
            "309\n",
            "cleaning now\n",
            "310\n",
            "cleaning now\n",
            "311\n",
            "cleaning now\n",
            "312\n",
            "cleaning now\n",
            "313\n",
            "cleaning now\n",
            "314\n",
            "cleaning now\n",
            "315\n",
            "cleaning now\n",
            "316\n",
            "cleaning now\n",
            "317\n",
            "cleaning now\n",
            "318\n",
            "cleaning now\n",
            "319\n",
            "cleaning now\n",
            "320\n",
            "cleaning now\n",
            "321\n",
            "cleaning now\n",
            "322\n",
            "cleaning now\n",
            "323\n",
            "cleaning now\n",
            "324\n",
            "cleaning now\n",
            "325\n",
            "cleaning now\n",
            "326\n",
            "cleaning now\n",
            "327\n",
            "cleaning now\n",
            "328\n",
            "cleaning now\n",
            "329\n",
            "cleaning now\n",
            "330\n",
            "cleaning now\n",
            "331\n",
            "cleaning now\n",
            "332\n",
            "cleaning now\n",
            "333\n",
            "cleaning now\n",
            "334\n",
            "cleaning now\n",
            "335\n",
            "cleaning now\n",
            "336\n",
            "cleaning now\n",
            "337\n",
            "cleaning now\n",
            "338\n",
            "cleaning now\n",
            "339\n",
            "cleaning now\n",
            "340\n",
            "cleaning now\n",
            "341\n",
            "cleaning now\n",
            "342\n",
            "cleaning now\n",
            "343\n",
            "cleaning now\n",
            "344\n",
            "cleaning now\n",
            "345\n",
            "cleaning now\n",
            "346\n",
            "cleaning now\n",
            "347\n",
            "cleaning now\n",
            "348\n",
            "cleaning now\n",
            "349\n",
            "cleaning now\n",
            "350\n",
            "cleaning now\n",
            "351\n",
            "cleaning now\n",
            "352\n",
            "cleaning now\n",
            "353\n",
            "cleaning now\n",
            "354\n",
            "cleaning now\n",
            "355\n",
            "cleaning now\n",
            "356\n",
            "cleaning now\n",
            "357\n",
            "cleaning now\n",
            "358\n",
            "cleaning now\n",
            "359\n",
            "cleaning now\n",
            "360\n",
            "cleaning now\n",
            "361\n",
            "cleaning now\n",
            "362\n",
            "cleaning now\n",
            "363\n",
            "excepted\n",
            "363\n",
            "364\n",
            "excepted\n",
            "364\n",
            "365\n",
            "cleaning now\n",
            "366\n",
            "cleaning now\n",
            "367\n",
            "cleaning now\n",
            "368\n",
            "cleaning now\n",
            "369\n",
            "cleaning now\n",
            "370\n",
            "cleaning now\n",
            "371\n",
            "cleaning now\n",
            "372\n",
            "cleaning now\n",
            "373\n",
            "cleaning now\n",
            "374\n",
            "cleaning now\n",
            "375\n",
            "cleaning now\n",
            "376\n",
            "cleaning now\n",
            "377\n",
            "cleaning now\n",
            "378\n",
            "cleaning now\n",
            "379\n",
            "excepted\n",
            "379\n",
            "380\n",
            "cleaning now\n",
            "381\n",
            "cleaning now\n",
            "382\n",
            "cleaning now\n",
            "383\n",
            "cleaning now\n",
            "384\n",
            "cleaning now\n",
            "385\n",
            "cleaning now\n",
            "386\n",
            "cleaning now\n",
            "387\n",
            "excepted\n",
            "387\n",
            "388\n",
            "cleaning now\n",
            "389\n",
            "cleaning now\n",
            "390\n",
            "excepted\n",
            "390\n",
            "391\n",
            "cleaning now\n",
            "392\n",
            "cleaning now\n",
            "393\n",
            "cleaning now\n",
            "394\n",
            "cleaning now\n",
            "395\n",
            "cleaning now\n",
            "396\n",
            "excepted\n",
            "396\n",
            "397\n",
            "cleaning now\n",
            "398\n",
            "excepted\n",
            "398\n",
            "399\n",
            "cleaning now\n",
            "400\n",
            "cleaning now\n",
            "401\n",
            "cleaning now\n",
            "402\n",
            "cleaning now\n",
            "403\n",
            "cleaning now\n",
            "404\n",
            "cleaning now\n",
            "405\n",
            "cleaning now\n",
            "406\n",
            "cleaning now\n",
            "407\n",
            "excepted\n",
            "407\n",
            "408\n",
            "cleaning now\n",
            "409\n",
            "excepted\n",
            "409\n",
            "410\n",
            "cleaning now\n",
            "411\n",
            "cleaning now\n",
            "412\n",
            "cleaning now\n",
            "413\n",
            "cleaning now\n",
            "414\n",
            "excepted\n",
            "414\n",
            "415\n",
            "cleaning now\n",
            "416\n",
            "cleaning now\n",
            "417\n",
            "excepted\n",
            "417\n",
            "418\n",
            "cleaning now\n",
            "419\n",
            "cleaning now\n",
            "420\n",
            "cleaning now\n",
            "421\n",
            "cleaning now\n",
            "422\n",
            "cleaning now\n",
            "423\n",
            "excepted\n",
            "423\n",
            "424\n",
            "cleaning now\n",
            "425\n",
            "cleaning now\n",
            "426\n",
            "excepted\n",
            "426\n",
            "427\n",
            "cleaning now\n",
            "428\n",
            "cleaning now\n",
            "429\n",
            "cleaning now\n",
            "430\n",
            "cleaning now\n",
            "431\n",
            "cleaning now\n",
            "432\n",
            "cleaning now\n",
            "433\n",
            "excepted\n",
            "433\n",
            "434\n",
            "cleaning now\n",
            "435\n",
            "cleaning now\n",
            "436\n",
            "excepted\n",
            "436\n",
            "437\n",
            "excepted\n",
            "437\n",
            "438\n",
            "excepted\n",
            "438\n",
            "439\n",
            "cleaning now\n",
            "440\n",
            "cleaning now\n",
            "441\n",
            "cleaning now\n",
            "442\n",
            "cleaning now\n",
            "443\n",
            "cleaning now\n",
            "444\n",
            "cleaning now\n",
            "445\n",
            "excepted\n",
            "445\n",
            "446\n",
            "cleaning now\n",
            "447\n",
            "cleaning now\n",
            "448\n",
            "cleaning now\n",
            "449\n",
            "cleaning now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSQOXg8rqv-f",
        "colab_type": "code",
        "outputId": "cff1d7d4-9b8f-4fac-e4aa-ad7ef724dd8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "for_tensor=df_2019.copy()\n",
        "for_tensor=for_tensor.drop(index=L_ex)\n",
        "#the following are needed when read from csv directly\n",
        "#for_tensor['Cast']=[ast.literal_eval(x) for x in for_tensor['Cast']]\n",
        "#for_tensor['Studio']=[ast.literal_eval(x) for x in for_tensor['Studio']]\n",
        "#for_tensor['Director']=[ast.literal_eval(x) for x in for_tensor['Director']]\n",
        "for_tensor.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opening</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>uni_code</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Studio</th>\n",
              "      <th>Subcategories 1</th>\n",
              "      <th>Subcategories 2</th>\n",
              "      <th>Subcategories 3</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>lower_thres</th>\n",
              "      <th>Week</th>\n",
              "      <th>Cast 1</th>\n",
              "      <th>Cast 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>October</td>\n",
              "      <td>05</td>\n",
              "      <td>Andhadhun</td>\n",
              "      <td>2iVYI99VGaw</td>\n",
              "      <td>Sriram Raghavan</td>\n",
              "      <td>Ayushman Khurana, Tabu, Radhika Apte,</td>\n",
              "      <td>Viacom 18 Motion Pictures</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>2018-09-14</td>\n",
              "      <td>40</td>\n",
              "      <td>ayushman khurana</td>\n",
              "      <td>tabu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>March</td>\n",
              "      <td>08</td>\n",
              "      <td>Badla</td>\n",
              "      <td>mSlgu8AQAd4</td>\n",
              "      <td>Sujoy Ghosh</td>\n",
              "      <td>Amitabh Bachchan, Taapsee Pannu,</td>\n",
              "      <td>Red Chillies Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>2019-02-15</td>\n",
              "      <td>10</td>\n",
              "      <td>amitabh bachchan</td>\n",
              "      <td>taapsee pannu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>January</td>\n",
              "      <td>11</td>\n",
              "      <td>Uri: The Surgical Strike</td>\n",
              "      <td>VVY3do673Zc</td>\n",
              "      <td>Aditya Dhar</td>\n",
              "      <td>Vicky Kaushal, Mohit Raina, Paresh Rawal, Yami...</td>\n",
              "      <td>RSVP Movies</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>2018-12-21</td>\n",
              "      <td>2</td>\n",
              "      <td>vicky kaushal</td>\n",
              "      <td>mohit raina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>March</td>\n",
              "      <td>23</td>\n",
              "      <td>Hichki</td>\n",
              "      <td>nLSaCFlXn-g</td>\n",
              "      <td>Siddharth P Malhotra</td>\n",
              "      <td>Rani Mukerji,</td>\n",
              "      <td>Yash Raj Films</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16</td>\n",
              "      <td>2018-03-02</td>\n",
              "      <td>12</td>\n",
              "      <td>rani mukerji</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>February</td>\n",
              "      <td>09</td>\n",
              "      <td>Pad Man</td>\n",
              "      <td>-K9ujx8vO_A</td>\n",
              "      <td>R. Balki</td>\n",
              "      <td>Akshay Kumar, Sonam Kapoor, Radhika Apte,</td>\n",
              "      <td>Columbia Pictures KriArj Entertainment</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>2018-01-19</td>\n",
              "      <td>6</td>\n",
              "      <td>akshay kumar</td>\n",
              "      <td>sonam kapoor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Opening Date  ...            Cast 1         Cast 2\n",
              "0   October   05  ...  ayushman khurana           tabu\n",
              "1     March   08  ...  amitabh bachchan  taapsee pannu\n",
              "2   January   11  ...     vicky kaushal    mohit raina\n",
              "3     March   23  ...      rani mukerji               \n",
              "4  February   09  ...      akshay kumar   sonam kapoor\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYCPsuEgqaMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final_list=list(for_tensor['top_words'])\n",
        "#words_set=set([item for sublist in final_list for item in sublist])\n",
        "\n",
        "\n",
        "#final_cast\n",
        "#for_tensor['Cast']=[x[0:1] for x in for_tensor['Cast']]\n",
        "final_cast=list(for_tensor['Cast 1'])\n",
        "final_cast=final_cast+list(for_tensor['Cast 2'])\n",
        "cast_set=set(final_cast)\n",
        "\n",
        "\n",
        "#director\n",
        "final_dir=list(for_tensor['Director'])\n",
        "#dir_set=set([item for sublist in final_dir for item in sublist])\n",
        "\n",
        "\n",
        "#production house\n",
        "#for_tensor['Studio']=[x[0:1] for x in for_tensor['Studio']]\n",
        "final_pro=list(for_tensor['Studio'])\n",
        "#pro_set=set([item for sublist in final_pro for item in sublist])\n",
        "words_set=set([])\n",
        "words_set.update(cast_set)\n",
        "words_set.update(final_pro)\n",
        "words_set.update(final_dir)\n",
        "\n",
        "words_set=list(words_set)\n",
        "\n",
        "word_ids=list(range(0,len(words_set)))\n",
        "\n",
        "dict_cov=dict(zip(words_set,word_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb5K05K8q91Z",
        "colab_type": "code",
        "outputId": "2c9c91c4-4ec6-4709-bce9-1404919cdd9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rows=[]\n",
        "columns=[]\n",
        "data=[]\n",
        "for i in range(0,len(for_tensor)):\n",
        "  print(i)\n",
        "  #words=for_tensor.iloc[i]['top_words']\n",
        "  #words=[dict_cov[x] for x in words]\n",
        "  #columns=columns+words\n",
        "\n",
        "  cast=[for_tensor.iloc[i]['Cast 1'],for_tensor.iloc[i]['Cast 2']]\n",
        "  cast=[dict_cov[x] for x in cast]\n",
        "  columns=columns+cast\n",
        "  \n",
        "  dire=[for_tensor.iloc[i]['Director']]\n",
        "  dire=[dict_cov[x] for x in dire]\n",
        "  columns=columns+dire\n",
        "\n",
        "  pro=[for_tensor.iloc[i]['Studio']]\n",
        "  pro=[dict_cov[x] for x in pro]\n",
        "  columns=columns+pro\n",
        "\n",
        "  n=len(dire)+len(cast)+len(pro)\n",
        "  row=[i]*n\n",
        "  vals=[1]*n\n",
        "\n",
        "  rows=rows+row\n",
        "  data=data+vals\n",
        "\n",
        "mat=sparse.coo_matrix((data,(rows,columns)),shape=(len(for_tensor),len(words_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsMJxOpvrBzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for_mat2=mat.toarray()\n",
        "s_t=np.concatenate((X_tfidf2,for_mat2),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyTgS2Vh1yhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tfidf2.shape\n",
        "org_mat = pd.DataFrame(data=X_tfidf, columns=['c'+str(i) for i in range(X_tfidf.shape[1])])\n",
        "org_mat.to_csv(\"For_randomforest.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCB4cJUU2HR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwkWt0Y2rLYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_result1 = np.array(for_tensor['Subcategories 1'])\n",
        "cat_result2 = np.array(for_tensor['Subcategories 2'])\n",
        "cat_result3 = np.array(for_tensor['Subcategories 3'])\n",
        "#org_mat = mat.toarray()\n",
        "org_mat_df = pd.DataFrame(data=s_t, columns=['c'+str(i) for i in range(s_t.shape[1])])\n",
        "org_mat_df.to_csv(\"/content/drive/My Drive/project/Results/model2_data_200.csv\")\n",
        "cat_result_df1 = pd.DataFrame({'t': cat_result1})\n",
        "cat_result_df1.to_csv(\"/content/drive/My Drive/project/Results/model2_data_target_1.csv\")\n",
        "cat_result_df2 = pd.DataFrame({'t': cat_result2})\n",
        "cat_result_df2.to_csv(\"/content/drive/My Drive/project/Results/model2_data_target_2.csv\")\n",
        "cat_result_df3 = pd.DataFrame({'t': cat_result3})\n",
        "cat_result_df3.to_csv(\"/content/drive/My Drive/project/Results/model2_data_target_3.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}